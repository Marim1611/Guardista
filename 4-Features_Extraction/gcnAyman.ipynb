{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["(4013,)\n","(2986, 33)\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, Sequential\n","import pickle\n","from torch_geometric.data import Dataset\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Input the Folders to train on\n","cve = '191'\n","pathToEdges = \"D:/ClassWork/Guardista/4-Features_Extraction/191/edges_191\"\n","\n","with open ('191/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","with open('191/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","    nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","print(features_matrices_list.shape)\n","#print(features_matrices_list)\n","print(features_matrices_list[0].shape)\n","\n","\n","torch.cuda.empty_cache()\n","\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for filename in os.listdir(pathToEdges):\n","    f = os.path.join(pathToEdges, filename)\n","    df = pd.read_csv(f, header=None)\n","    df.dropna(inplace=True)             # <---- here is the dropping\n","    adj_Lists.append(df)\n","    \n","# Reading the label of each node\n","Targets_List = []\n","for target in nodes_targets_list:\n","    Targets_List.append(np.array(target, dtype='int64'))\n","    \n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","\n","\n","assert (len(adj_Lists) == len(Targets_List))\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","\n","#A set to know the number of classes\n","numClasses = set()\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    numClasses.update(nodes_targets_list[i])\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor(nodes_targets_list[i]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        print('DIMENSION ERROR')\n","        print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","numClasses = len(numClasses)\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","print(f\"total number of files {len(adj_Tensors)}\")\n","print(f\"number of files to be trained on = {len(adj_Tensors) - cntCorruptData}\")\n","print(f\"we Have {numClasses} Classes\")\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 457 CVE"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","# Input the Folders to train on\n","cve = '457'\n","pathToEdges = \"D:/ClassWork/Guardista/4-Features_Extraction/457/edges_457\"\n","\n","with open ('457/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","with open('457/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","    nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","print(features_matrices_list.shape)\n","#print(features_matrices_list)\n","print(features_matrices_list[0].shape)\n","\n","\n","torch.cuda.empty_cache()\n","\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for filename in os.listdir(pathToEdges):\n","    f = os.path.join(pathToEdges, filename)\n","    df = pd.read_csv(f, header=None)\n","    df.dropna(inplace=True)             # <---- here is the dropping\n","    adj_Lists.append(df)\n","    \n","# Reading the label of each node\n","Targets_List = []\n","for target in nodes_targets_list:\n","    encodedTarget = [2 if i else 0 for i in target]\n","    Targets_List.append(np.array(encodedTarget, dtype='int64'))\n","    \n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","\n","\n","assert (len(adj_Lists) == len(Targets_List))\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","\n","\n","#A set to know the number of classes\n","numClasses = set()\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    numClasses.update(nodes_targets_list[i])\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor(nodes_targets_list[i]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        print('DIMENSION ERROR')\n","        print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","numClasses = 3\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","print(f\"total number of files {len(adj_Tensors)}\")\n","print(f\"number of files to be trained on = {len(adj_Tensors) - cntCorruptData}\")\n","print(f\"we Have {numClasses} Classes\")\n","\n","\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.3)\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Reading CSVs and npy files\n","#### NOTICE that the CSV contains some nodes having edges going out to nowhere, I DROP THOSE EDGES COMPLETELY<br>this drop seems to cause some nodes to disappear forever\n","\n","\n","#### Also notice that some nodes in the edges' file don't have corresponding node features in the npy file"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.404729Z","iopub.status.busy":"2023-06-26T10:37:45.404418Z","iopub.status.idle":"2023-06-26T10:37:46.026286Z","shell.execute_reply":"2023-06-26T10:37:46.025336Z","shell.execute_reply.started":"2023-06-26T10:37:45.404704Z"},"trusted":true},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Mapping each node in the adjacency list, to its node ID in the node features<br>It is done by getting all unique nodes from the adjacency list, sorting them, then renaming each of them starting from 0"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:46.028755Z","iopub.status.busy":"2023-06-26T10:37:46.028396Z","iopub.status.idle":"2023-06-26T10:37:47.409595Z","shell.execute_reply":"2023-06-26T10:37:47.408479Z","shell.execute_reply.started":"2023-06-26T10:37:46.028723Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\n# Determine the unique node IDs in the edge list\\nappender= np.concatenate([adj_Lists[i][0].to_numpy() for i in range(len(adj_Lists))])\\nappender2 = np.concatenate([adj_Lists[i][1].to_numpy() for i in range(len(adj_Lists))])\\nunique_node_ids = np.unique(np.concatenate((appender, appender2)))\\n\\n# Create a dictionary that maps the old node IDs to new 0-based indices\\nnode_id_to_index = {node_id: i for i, node_id in enumerate(unique_node_ids)}\\n\\n# Convert the edge list to use 0-based indices\\nadj_Tensors = []\\nfor adj in adj_Lists:\\n    adj2 = pd.DataFrame(data=adj)\\n    adj2.iloc[:,0] = adj.iloc[:,0].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\\n    adj2.iloc[:,1] = adj.iloc[:,1].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\\n\\n    # the documentation strictly says to convert the adjacency list to a contiguous list\\n    adj_Tensors.append(torch.as_tensor(adj2.to_numpy(), dtype=torch.long).t().contiguous())     \\n    \\n'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["\"\"\"\n","# Determine the unique node IDs in the edge list\n","appender= np.concatenate([adj_Lists[i][0].to_numpy() for i in range(len(adj_Lists))])\n","appender2 = np.concatenate([adj_Lists[i][1].to_numpy() for i in range(len(adj_Lists))])\n","unique_node_ids = np.unique(np.concatenate((appender, appender2)))\n","\n","# Create a dictionary that maps the old node IDs to new 0-based indices\n","node_id_to_index = {node_id: i for i, node_id in enumerate(unique_node_ids)}\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    adj2 = pd.DataFrame(data=adj)\n","    adj2.iloc[:,0] = adj.iloc[:,0].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","    adj2.iloc[:,1] = adj.iloc[:,1].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj2.to_numpy(), dtype=torch.long).t().contiguous())     \n","    \n","\"\"\"\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Finalizing our input dataset"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:47.411283Z","iopub.status.busy":"2023-06-26T10:37:47.410917Z","iopub.status.idle":"2023-06-26T10:37:47.537970Z","shell.execute_reply":"2023-06-26T10:37:47.536976Z","shell.execute_reply.started":"2023-06-26T10:37:47.411250Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 4013\n","number of files to be trained on = 4013\n","we Have 2 Classes\n"]}],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:49:10.223288Z","iopub.status.busy":"2023-06-26T10:49:10.222397Z","iopub.status.idle":"2023-06-26T10:49:31.983085Z","shell.execute_reply":"2023-06-26T10:49:31.981783Z","shell.execute_reply.started":"2023-06-26T10:49:10.223243Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(33, 60)\n","  (conv2): GCNConv(60, 50)\n","  (conv3): GCNConv(50, 40)\n","  (conv4): GCNConv(40, 16)\n","  (conv5): GCNConv(16, 2)\n",")\n","Epoch: 001, Loss: 0.0403, epoch Accuracy: 0.9170\n"]}],"source":["\n","# Customize this as you wish, just make sure the first conv layer takes feature_dimension, and the last hidden conv layer outputs num_classes\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels , feature_dimension, num_classes):\n","        super().__init__()\n","        self.conv1 = GCNConv(feature_dimension, 60)\n","        self.conv2 = GCNConv(60, 50)\n","        self.conv3 = GCNConv(50, 40)\n","        self.conv4 = GCNConv(40, 16)\n","        self.conv5 = GCNConv(16, num_classes)\n","        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.005, weight_decay=5e-4)\n","    \n","    def forward(self, x, edge_index):\n","        # x: Node feature matrix \n","        # edge_index: Graph connectivity matrix \n","        x = self.conv1(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv2(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv3(x, edge_index)\n","        x = F.relu(x)\n","        x = self.conv4(x, edge_index)\n","        x = F.relu(x)\n","        x = F.dropout(x, training=self.training)\n","        Embedding = x\n","        x = self.conv5(x, edge_index)\n","        return x, F.log_softmax(x, dim=1), Embedding   # X is used for the loss computation,  F.log_softmax is the classification, Embedding is the emb\n","\n","    \n","    \n","    \n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = GCN(hidden_channels=30, feature_dimension=33, num_classes=2).to(device=device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-5)\n","penaltyWeights = torch.tensor([0.50102, 1000.5859])\n","criterion = torch.nn.CrossEntropyLoss(weight=penaltyWeights.to(device=device))\n","\n","def train(model, data):\n","    model.train()\n","    optimizer.zero_grad()  # Clear gradients.\n","    \n","    out = model(data.x.to(device=device), data.edge_index.to(device=device))  # Perform a single forward pass.\n","    loss = criterion(out[1], data.y.to(device=device))  # Compute the loss solely based on the training nodes.\n","    loss.backward()  # Derive gradients.\n","    optimizer.step()  # Update parameters based on gradients.\n","    embOut = out[2]\n","    return loss, embOut\n","\n","\n","\n","def test(model, data):\n","    model.eval()\n","    out = model(data.x.to(device=device), data.edge_index.to(device=device))\n","    pred = out[1].argmax(dim=1)  # Use the class with highest probability.\n","    test_correct = pred == data.y  # Check against ground-truth labels for test nodes.\n","    test_acc = int(test_correct.sum()) / int(len(data.x))  # Derive ratio of correct predictions.\n","\n","    EmbOutput = out[2]\n","    return test_acc, pred, EmbOutput\n","\n","\n","# Training\n","for epoch in range(1, 2):\n","    acc_accum = 0\n","    for bat, batchData in enumerate(loader):\n","        for i in range (len(batchData)):\n","            loss,_ = train(model, batchData[i].to(device=device))\n","            test_acc,_,_ = test(model, batchData[i].to(device=device))\n","            acc_accum += test_acc\n","    avg_acc = acc_accum / (batch_size*len(loader))\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, epoch Accuracy: {avg_acc:.4f}')\n","\n","\n","\n","# Saving the model\n","#with open('GCN.pkl', 'wb') as f:\n","#    pickle.dump(model, f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      0.99      0.99   2682307\n","           1       0.13      1.00      0.23      5459\n","\n","    accuracy                           0.99   2687766\n","   macro avg       0.57      0.99      0.61   2687766\n","weighted avg       1.00      0.99      0.99   2687766\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","y_true = []\n","y_predicted = []\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    _,pred,_ = test(model, d.to(device=device))\n","    y_predicted.append(pred.cpu().numpy())\n","\n","\n","y_true = np.concatenate([y_true[i] for i in range(len(y_true))])\n","y_predicted = np.concatenate([y_predicted[i] for i in range(len(y_predicted))])\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing on Entire file Classification"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           1       1.00      1.00      1.00      1204\n","\n","    accuracy                           1.00      1204\n","   macro avg       1.00      1.00      1.00      1204\n","weighted avg       1.00      1.00      1.00      1204\n","\n"]}],"source":["y_true = []\n","y_predicted = []\n","for d in allData_test:\n","    if(np.sum(d.y.cpu().numpy(), dtype=np.int32) > 0):\n","        y_true.append(1)\n","    else:\n","        y_true.append(0)\n","    \n","    _,pred,_ = test(model, d.to(device=device))\n","    if(np.sum(pred.cpu().numpy(), dtype=np.int32) > 0):\n","        y_predicted.append(1)\n","    else:\n","        y_predicted.append(0)\n","    \n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference\n","### First, Let's define some a function encapsulating all the preprocessing steps done<br>FOR A SINGLE GRAPH ONLY"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from statFeaturesUtil import features_per_graph_per_node\n","\n","def preProcessingOneDataPoint(pathToUserNodes, pathToUserEdges):\n","\n","    print(pathToUserNodes)\n","    features_matrices_list = features_per_graph_per_node(pathToUserNodes)\n","\n","\n","    for filename in os.listdir(pathToUserEdges):\n","        f = str(os.path.join(pathToUserEdges, filename)).replace('\\\\', '/')\n","        pddf = pd.read_csv(f, header=None)\n","        \n","        pddf.dropna(inplace=True)\n","        #df.rename(index={0: \"source\", 1: \"target\"})\n","        adj_Lists = pddf\n","        print(pddf.head())\n","\n","        \n","    \n","    n = np.array(features_matrices_list[0],dtype='int64')\n","    node_Features_List = torch.tensor(n, dtype=torch.float)\n","\n","\n","\n","    # Determine the unique node IDs in the edge list\n","    unique_node_ids=  np.unique(np.concatenate((adj_Lists.iloc[:, 0].to_numpy(), adj_Lists.iloc[:, 1].to_numpy())  ))\n","\n","    # Create a dictionary that maps the old node IDs to new 0-based indices\n","    node_id_to_index = {node_id: i for i, node_id in enumerate(unique_node_ids)}\n","\n","    # Convert the edge list to use 0-based indices\n","    \n","        \n","    #print(df[df.columns[1]].isna().sum())\n","    if(adj_Lists.loc[:,1].isna().any()):\n","        print(adj_Lists[adj_Lists[adj_Lists.columns[1]].isna()])\n","    #print(adj[:,0])\n","    \n","    adj2 = pd.DataFrame(data=adj_Lists)\n","    adj2.iloc[:,0] = adj_Lists.iloc[:,0].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","    adj2.iloc[:,1] = adj_Lists.iloc[:,1].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","\n","    adj_Tensors = torch.as_tensor(adj2.to_numpy(), dtype=torch.long).t().contiguous()\n","\n","\n","        \n","\n","    d = Data(x=node_Features_List, edge_index=adj_Tensors, y=0)\n","    try:\n","        d.validate(raise_on_error=True)\n","    except:\n","        print('DIMENSION ERROR')\n","        print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Nodes\")\n","        return False\n","    \n","\n","    return d"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### You input 2 folders, one folder containing the json of nodes, the other contains the csv for edges.<br>don't mix them in one folder, bad things happen"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","pathToUser_Nodes = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\tst\\nod\".replace('\\\\', '/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/tst/edg\".replace('\\\\', '/')\n","\n","inputPoint = preProcessingOneDataPoint(pathToUser_Nodes, pathToUser_Edges)\n","with open('GCN.pkl', 'rb') as f:\n","    model = pickle.load(f)\n","_,classification,Embedding = test(model, inputPoint)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(classification)\n","\n","print(Embedding)\n","\n","print(Embedding.shape)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
