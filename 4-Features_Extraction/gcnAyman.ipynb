{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, Sequential\n","import pickle\n","from torch_geometric.data import Dataset\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","from gcnHelpers import GCN, preProcessingOneDataPoint, train, test"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training on Multiple classes\n","#### please put the folders of the CVE's in the script's directory<br>then inside each folder put 2 subfolders, one for the nodes and the other for edges"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 121------------------\n","num Graphs : 5940\n","num nodes : 12564642 of which 20675 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 5940\n","number of files to be trained on = 5940\n","\n","\n","-----------LOADING CVE 191------------------\n","num Graphs : 4013\n","num nodes : 8937258 of which 18167 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 4013\n","number of files to be trained on = 4013\n","\n","\n","-----------LOADING CVE 401------------------\n","num Graphs : 2261\n","num nodes : 4553383 of which 8766 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 2261\n","number of files to be trained on = 2261\n","\n","\n","-----------LOADING CVE 457------------------\n","num Graphs : 914\n","num nodes : 1765018 of which 4138 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 914\n","number of files to be trained on = 914\n","\n","\n"]}],"source":["CVES_to_train_on = ['121', '191', '401', '457']\n","\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","\n","torch.cuda.empty_cache()\n","\n","for encClass, cve in enumerate(CVES_to_train_on):\n","    print(f\"-----------LOADING CVE {cve}------------------\")\n","\n","    \n","    pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}/edges_{cve}\"\n","\n","    with open (f'{cve}/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","        features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","    with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","        nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","    \n","    print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","\n","\n","    # Reading the adjacency list of each graph\n","    adj_Lists = []\n","    for filename in os.listdir(pathToEdges):\n","        f = os.path.join(pathToEdges, filename)\n","        df = pd.read_csv(f, header=None)\n","        df.dropna(inplace=True)             # <---- here is the dropping\n","        adj_Lists.append(df)\n","        \n","\n","\n","    numVulnNodes = 0\n","\n","    encoded = encClass + 1\n","    # Reading the label of each node\n","    Targets_List = []\n","    for target in nodes_targets_list:\n","        numVulnNodes += sum(target)\n","        encodedTarget = [encoded if i==1 or i == '1' else 0 for i in target]\n","        Targets_List.append(np.array(encodedTarget, dtype='int64'))\n","        \n","\n","    numNodes = 0\n","    # Reading the features of each node\n","    node_Features_List = []\n","    for node_feature in features_matrices_list:\n","        n = np.array(node_feature,dtype='int64')\n","        numNodes+= n.shape[0]\n","        node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","    assert (len(adj_Lists) == len(Targets_List))\n","    assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","    # Convert the edge list to use 0-based indices\n","    adj_Tensors = []\n","    for adj in adj_Lists:\n","        # the documentation strictly says to convert the adjacency list to a contiguous list\n","        adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","    #Iterate over each graph, make a Data object, then append to all our dataset\n","    cntCorruptData=0\n","    for i, adj in enumerate(adj_Tensors):\n","        d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor(Targets_List[i]))  # <--- from the documentation.\n","        try:\n","            d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","        except:\n","            print('DIMENSION ERROR')\n","            print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","            print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","            cntCorruptData +=1\n","            continue\n","        allData.append(d)\n","\n","\n","    print(f\"num nodes : {numNodes} of which {numVulnNodes} are vulnerable\")\n","    print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","    print(f\"total number of files {len(adj_Tensors)}\")\n","    print(f\"number of files to be trained on = {len(adj_Tensors) - cntCorruptData}\\n\\n\")\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train-Test-Split and data loader"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes {0, 1, 2, 3, 4}\n"]}],"source":["\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.3, shuffle=True)\n","\n","unique_classes = set()\n","\n","for d in allData_train:\n","    unique_classes.update(d.y.numpy())\n","\n","print(f\"Unique classes {unique_classes}\")\n","\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Reading CSVs and npy files\n","#### NOTICE that the CSV contains some nodes having edges going out to nowhere, I DROP THOSE EDGES COMPLETELY<br>this drop seems to cause some nodes to disappear forever\n","\n","\n","#### Also notice that some nodes in the edges' file don't have corresponding node features in the npy file"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Mapping each node in the adjacency list, to its node ID in the node features<br>It is done by getting all unique nodes from the adjacency list, sorting them, then renaming each of them starting from 0"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:46.028755Z","iopub.status.busy":"2023-06-26T10:37:46.028396Z","iopub.status.idle":"2023-06-26T10:37:47.409595Z","shell.execute_reply":"2023-06-26T10:37:47.408479Z","shell.execute_reply.started":"2023-06-26T10:37:46.028723Z"},"trusted":true},"outputs":[],"source":["\"\"\"\n","# Determine the unique node IDs in the edge list\n","appender= np.concatenate([adj_Lists[i][0].to_numpy() for i in range(len(adj_Lists))])\n","appender2 = np.concatenate([adj_Lists[i][1].to_numpy() for i in range(len(adj_Lists))])\n","unique_node_ids = np.unique(np.concatenate((appender, appender2)))\n","\n","# Create a dictionary that maps the old node IDs to new 0-based indices\n","node_id_to_index = {node_id: i for i, node_id in enumerate(unique_node_ids)}\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    adj2 = pd.DataFrame(data=adj)\n","    adj2.iloc[:,0] = adj.iloc[:,0].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","    adj2.iloc[:,1] = adj.iloc[:,1].apply(lambda x: -1 if x==-1 else node_id_to_index [int(x)])\n","\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj2.to_numpy(), dtype=torch.long).t().contiguous())     \n","    \n","\"\"\"\n","    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class\n","#### If you want to change something, refer to gcnHelpers.py<br>that script contains literally anything related to the GCN architecture"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:49:10.223288Z","iopub.status.busy":"2023-06-26T10:49:10.222397Z","iopub.status.idle":"2023-06-26T10:49:31.983085Z","shell.execute_reply":"2023-06-26T10:49:31.981783Z","shell.execute_reply.started":"2023-06-26T10:49:10.223243Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(33, 60)\n","  (conv2): GCNConv(60, 60)\n","  (conv3): GCNConv(60, 30)\n","  (conv4): GCNConv(30, 16)\n","  (conv5): GCNConv(16, 5)\n",")\n","Epoch: 001, Loss: 0.1857, epoch Accuracy: 0.9247\n","Epoch: 002, Loss: 1.6210, epoch Accuracy: 0.9496\n","Epoch: 003, Loss: 0.0612, epoch Accuracy: 0.9604\n","Epoch: 004, Loss: 0.0480, epoch Accuracy: 0.9635\n","Epoch: 005, Loss: 0.0402, epoch Accuracy: 0.9664\n","Epoch: 006, Loss: 0.4948, epoch Accuracy: 0.9680\n","Epoch: 007, Loss: 0.0558, epoch Accuracy: 0.9713\n","Epoch: 008, Loss: 0.3413, epoch Accuracy: 0.9723\n","Epoch: 009, Loss: 0.0649, epoch Accuracy: 0.9741\n","Epoch: 010, Loss: 0.1531, epoch Accuracy: 0.9750\n","Epoch: 011, Loss: 0.0263, epoch Accuracy: 0.9758\n"]}],"source":["\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = GCN(hidden_channels=30, feature_dimension=33, lr=0.001, num_classes=5).to(device=device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-5)\n","penaltyWeights = torch.tensor( [    1.0018,      1355.54 ,  1533.44,     3153.71,       7580.941  ])  # <--- THOSE WEIGHTS ARE COMPUTED MANUALLY ACC TO AN EQUATION\n","criterion = torch.nn.CrossEntropyLoss(weight=penaltyWeights.to(device=device))\n","\n","\n","\n","\n","\n","# Training\n","for epoch in range(1, 12):\n","    acc_accum = 0\n","    for bat, batchData in enumerate(loader):\n","        for i in range (len(batchData)):\n","            loss,_ = train(model, batchData[i].to(device=device), optimizer=optimizer, criterion= criterion, device=device)\n","            test_acc,_,_ = test(model, batchData[i].to(device=device), device=device)\n","            acc_accum += test_acc\n","    avg_acc = acc_accum / (batch_size*len(loader))\n","    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, epoch Accuracy: {avg_acc:.4f}')\n","\n","\n","\n","# Saving the model\n","with open('GCN.pkl', 'wb') as f:\n","    pickle.dump(model.cpu(), f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{0, 1, 2, 3, 4}\n","              precision    recall  f1-score   support\n","\n","           0       1.00      0.98      0.99   8349182\n","           1       0.08      0.88      0.15      6401\n","           2       0.09      0.96      0.16      5363\n","           3       0.03      0.94      0.06      2561\n","           4       0.13      0.82      0.22      1304\n","\n","    accuracy                           0.98   8364811\n","   macro avg       0.27      0.91      0.32   8364811\n","weighted avg       1.00      0.98      0.99   8364811\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device=device)\n","\n","y_true = []\n","y_predicted = []\n","unique_classes = set()\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    unique_classes.update(list(d.y.cpu().numpy()))\n","    _,pred,_ = test(model, d.to(device=device), device=device)\n","    y_predicted.append(pred.cpu().numpy())\n","\n","print(unique_classes)\n","\n","y_true = np.concatenate([y_true[i] for i in range(len(y_true))])\n","y_predicted = np.concatenate([y_predicted[i] for i in range(len(y_predicted))])\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing on Entire file Classification"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           1       0.94      0.87      0.90      1814\n","           2       0.95      0.86      0.90      1176\n","           3       0.65      0.98      0.78       663\n","           4       1.00      0.63      0.77       286\n","\n","    accuracy                           0.87      3939\n","   macro avg       0.88      0.84      0.84      3939\n","weighted avg       0.90      0.87      0.87      3939\n","\n"]}],"source":["y_true = []\n","y_predicted = []\n","for d in allData_test:\n","    lable = np.max(d.y.cpu().numpy())\n","    if(lable == 1):\n","        y_true.append(1)\n","    elif(lable == 2):\n","        y_true.append(2)\n","    elif(lable == 3):\n","        y_true.append(3)\n","    elif(lable == 4):\n","        y_true.append(4)\n","\n","    \n","    _,pred,_ = test(model, d.to(device=device), device=device)\n","    counts = np.bincount(pred.cpu().numpy())\n","\n","    (count_0, count_1, count_2, count_3, count_4) =    (counts[0],\\\n","                                            0 if len(counts) < 2 else counts[1],\\\n","                                            0 if len(counts) < 3 else counts[2],\\\n","                                            0 if len(counts) < 4 else counts[3], 0 if len(counts) < 5 else counts[4])            #Those were sanity (size) checks\n","    y_predicted.append( np.argmax(np.array([count_1, count_2, count_3, count_4])) + 1)\n","    \n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### You input 2 folders, one folder containing the json of nodes, the other contains the csv for edges.<br>don't mix them in one folder, bad things happen"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import gcnHelpers\n","import numpy as np\n","\n","pathToUser_Nodes = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\tst\\nod\".replace('\\\\', '/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/tst/edg\".replace('\\\\', '/')\n","classification, embeddings, AvgPooled, MaxPooled = gcnHelpers.InferenceGCN(pathToUser_Nodes, pathToUser_Edges, pathToUser_Nodes, 'false') # <---- Notice I lazily put the output folder to user nodes"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[0 0 0 ... 0 0 0]\n","------------------------------------------------\n","[[0.         0.         3.8064592  ... 3.821001   5.447873   0.        ]\n"," [0.35570067 0.         5.956406   ... 5.3900223  5.8631268  0.        ]\n"," [0.         0.         8.600882   ... 1.3311294  4.239421   0.        ]\n"," ...\n"," [0.         0.         5.048689   ... 5.5136294  5.5096493  0.        ]\n"," [1.3681096  0.         4.0339794  ... 4.427201   6.1081123  0.        ]\n"," [0.27392727 0.         5.350784   ... 5.0768337  5.245433   0.        ]]\n","------------------------------------------------\n","[0.784646   0.08142199 6.143399   5.2373586  5.606539   6.9538674\n"," 0.01934045 9.283028   0.11684757 0.06016934 0.11146666 8.869153\n"," 8.912067   6.3315077  9.01263    0.05021806]\n","------------------------------------------------\n","[ 25.076975    2.29146    64.17055    61.578133   50.404213   66.48347\n","   2.041499  166.96599     3.8428469   2.3604486   6.6041737  95.02131\n"," 130.35931    69.392456  129.0811      3.4771945]\n"]}],"source":["print(classification)\n","print('------------------------------------------------')\n","print(embeddings)\n","print('------------------------------------------------')\n","print(AvgPooled)\n","print('------------------------------------------------')\n","print(MaxPooled)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
