{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stellargraph.mapper import (\n",
    "    CorruptedGenerator,\n",
    "    FullBatchNodeGenerator,\n",
    "    GraphSAGENodeGenerator,\n",
    "    HinSAGENodeGenerator,\n",
    "    ClusterNodeGenerator,\n",
    ")\n",
    "from stellargraph import StellarGraph\n",
    "from stellargraph.layer import GCN, DeepGraphInfomax, GraphSAGE, GAT, APPNP, HinSAGE\n",
    "\n",
    "from stellargraph import datasets\n",
    "from stellargraph.utils import plot_history\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.manifold import TSNE\n",
    "# from IPython.display import display, HTML\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# PUT CVE CLASS\n",
    "# cve = 'SAFE_23'\n",
    "# edges_directory_path = 'CFGs/'+ cve +'_CFGs/edges_' + cve\n",
    "edges_directory_path = sys.argv[1]\n",
    "\n",
    "cve = 121\n",
    "\n",
    "\n",
    "# PUT CVE CLASS\n",
    "with open('/kaggle/input/features_matrices_' + cve + '.npy', 'rb') as f:\n",
    "    features_matrices_list = np.load(f,  allow_pickle=True)\n",
    "\n",
    "with open('/kaggle/input/nodes_targets_' + cve + '.npy', 'rb') as f:\n",
    "    nodes_targets_list = np.load(f,  allow_pickle=True)\n",
    "\n",
    "\n",
    "\n",
    "print('number of matrices', len(features_matrices_list))\n",
    "\n",
    "graphs_representaions_path = '/graphs_representations_'+ cve +'.csv'\n",
    "\n",
    "features_number = features_matrices_list[0].shape[1] #not used bec the embeddings size turns out to be 128\n",
    "\n",
    "features_names = []\n",
    "for i in range(128):\n",
    "    features_names.append('X'+str(i))\n",
    "features_names.append('y')\n",
    "\n",
    "#if not os.path.exists(out_path):\n",
    "#    os.mkdir(out_path)\n",
    "\n",
    "# Using a with open() statement will automatically close a file once the block has completed\n",
    "#if not os.path.exists(out_path + graphs_representaions_path):\n",
    "#    with open(out_path + graphs_representaions_path, 'a', newline='') as file:\n",
    "#            writer = csv.writer(file)\n",
    "#            writer.writerow(features_names)\n",
    "\n",
    "\n",
    "i = 0\n",
    "graphs = []\n",
    "for filename in os.listdir(edges_directory_path):\n",
    "    f = os.path.join(edges_directory_path, filename)\n",
    "\n",
    "    if os.path.isfile(f) and f.endswith('.csv'):\n",
    "        # calling clear_session() when creating models in a loop to reset all state generated by Keras.\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "        # print(f)\n",
    "\n",
    "        features_matrix = features_matrices_list[i]\n",
    "        nodes_targets = nodes_targets_list[i]\n",
    "        \n",
    "        # Read in edges\n",
    "        # if os.path.getsize(f) > 1000: \n",
    "        if os.stat(f).st_size > 0:\n",
    "            edges = pd.read_csv(f)\n",
    "            edges.columns = ['source', 'target'] # renaming for StellarGraph compatibility\n",
    "            col=np.array(edges['target'], np.int16)\n",
    "            edges['target']=col\n",
    "            # print(edges)\n",
    "            # print(edges.shape)\n",
    "\n",
    "            node_features = pd.DataFrame(features_matrix)\n",
    "            # print(node_features)\n",
    "            # print(node_features.shape)\n",
    "\n",
    "            G = StellarGraph(node_features, edges)\n",
    "            graphs.append(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn import GCNConv\n",
    "import numpy as np\n",
    "import stellargraph as sg\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, 16)\n",
    "        self.conv2 = GCNConv(16, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = torch.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Load the graph data and create a PyTorch Geometric DataLoader\n",
    "def load_data(graphs):\n",
    "    dataset = []\n",
    "    for graph in graphs:\n",
    "        node_features = graph.node_features()\n",
    "        edge_index = torch.tensor(graph.to_adjacency_matrix().todense().nonzero(), dtype=torch.long)\n",
    "        y = torch.tensor(graph.node_type, dtype=torch.long)\n",
    "        x = torch.tensor(node_features.values, dtype=torch.float)\n",
    "        data = Data(x=x, edge_index=edge_index, y=y)\n",
    "        dataset.append(data)\n",
    "    loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    return loader\n",
    "\n",
    "# Train the GCN model on the graph data\n",
    "def train_gcn(graphs):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = GCN(in_channels=10, out_channels=2).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loader = load_data(graphs)\n",
    "    for epoch in range(10):\n",
    "        for batch in loader:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            out = model(batch.x, batch.edge_index)\n",
    "            loss = torch.nn.functional.cross_entropy(out, batch.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_gcn(graphs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
