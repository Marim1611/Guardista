{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Graph Classification"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, Sequential, Linear, global_mean_pool\n","import pickle\n","from torch_geometric.data import Dataset\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n","from sklearn.model_selection import train_test_split\n","\n","from gcnHelpers import *\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training on Multiple classes\n","#### please put the folders of the CVE's in the script's directory<br>then inside each folder put 2 subfolders, one for the nodes and the other for edges\n","##### example: folder 121 contains 4 subfolders (nodes_121, edges_121, feature_matrices, nodes_targets)<br>and feature_matrices and nodes_targets contain the npy files"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 121------------------\n","num Graphs : 5940\n","num nodes : 12564642 of which 20675 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 5940\n","number of files to be trained on = 5940\n","\n","\n","-----------LOADING CVE 191------------------\n","num Graphs : 4013\n","num nodes : 8937258 of which 18167 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 4013\n","number of files to be trained on = 4013\n","\n","\n","-----------LOADING CVE 401------------------\n","num Graphs : 2261\n","num nodes : 4553383 of which 8766 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 2261\n","number of files to be trained on = 2261\n","\n","\n","-----------LOADING CVE 457------------------\n","num Graphs : 914\n","num nodes : 1765018 of which 4138 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 914\n","number of files to be trained on = 914\n","\n","\n"]}],"source":["CVES_to_train_on = ['121', '191', '401', '457']\n","\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","numGraphs_List = []\n","\n","torch.cuda.empty_cache()\n","\n","for encClass, cve in enumerate(CVES_to_train_on):\n","    print(f\"-----------LOADING CVE {cve}------------------\")\n","\n","    \n","    pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}/edges_{cve}\"\n","\n","    with open (f'{cve}/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","        features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","    with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","        nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","    \n","    print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","    numGraphs_List.append(features_matrices_list.shape[0])\n","\n","\n","    # Reading the adjacency list of each graph\n","    adj_Lists = []\n","    for filename in os.listdir(pathToEdges):\n","        f = os.path.join(pathToEdges, filename)\n","        df = pd.read_csv(f, header=None)\n","        df.dropna(inplace=True)             # <---- here is the dropping\n","        adj_Lists.append(df)\n","        \n","\n","\n","    numVulnNodes = 0\n","\n","    encoded = encClass\n","    # Reading the label of each node\n","    Targets_List = []\n","    for target in nodes_targets_list:\n","        numVulnNodes += sum(target)\n","        Targets_List.append(encoded)\n","        \n","\n","    numNodes = 0\n","    # Reading the features of each node\n","    node_Features_List = []\n","    for node_feature in features_matrices_list:\n","        n = np.array(node_feature,dtype='int64')\n","        numNodes+= n.shape[0]\n","        node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","    assert (len(adj_Lists) == len(Targets_List))\n","    assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","    # Convert the edge list to use 0-based indices\n","    adj_Tensors = []\n","    for adj in adj_Lists:\n","        # the documentation strictly says to convert the adjacency list to a contiguous list\n","        adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","    #Iterate over each graph, make a Data object, then append to all our dataset\n","    cntCorruptData=0\n","    for i, adj in enumerate(adj_Tensors):\n","        d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor(Targets_List[i]))  # <--- from the documentation.\n","        try:\n","            d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","        except:\n","            print('DIMENSION ERROR')\n","            print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","            print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","            cntCorruptData +=1\n","            continue\n","        allData.append(d)\n","\n","\n","    print(f\"num nodes : {numNodes} of which {numVulnNodes} are vulnerable\")\n","    print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","    print(f\"total number of files {len(adj_Tensors)}\")\n","    print(f\"number of files to be trained on = {len(adj_Tensors) - cntCorruptData}\\n\\n\")\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Computing class weights to be used in a weighted loss function"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Weights for each class are:\n"," [2.21010101010101, 3.2713680538250687, 5.806280406899602, 14.36323851203501]\n"]}],"source":["total_Graphs = sum(numGraphs_List)\n","Class_Weights = [total_Graphs/i for i in numGraphs_List]\n","print(f\"Weights for each class are:\\n {Class_Weights}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train-Val-Test-Split and data loader"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes {0, 1, 2, 3}\n"]}],"source":["\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.2, shuffle=True)\n","\n","# Train Validation Split\n","allData_train, allData_val = train_test_split(allData_train, test_size=0.1, shuffle=True)\n","\n","unique_classes = set()\n","\n","for d in allData_train:\n","    unique_classes.add(int(d.y.numpy()))\n","\n","print(f\"Unique classes {unique_classes}\")\n","\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(allData_test, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(allData_val, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class\n","#### anything related to the architecture"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:49:10.223288Z","iopub.status.busy":"2023-06-26T10:49:10.222397Z","iopub.status.idle":"2023-06-26T10:49:31.983085Z","shell.execute_reply":"2023-06-26T10:49:31.981783Z","shell.execute_reply.started":"2023-06-26T10:49:10.223243Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(33, 64)\n","  (conv2): GCNConv(64, 64)\n","  (conv3): GCNConv(64, 64)\n","  (lin): Linear(64, 4, bias=True)\n",")\n","Epoch: 001, Train Acc: 0.4984, Validation Acc: 0.5090\n","Epoch: 002, Train Acc: 0.7621, Validation Acc: 0.7536\n","Epoch: 003, Train Acc: 0.7828, Validation Acc: 0.7840\n","Epoch: 004, Train Acc: 0.7456, Validation Acc: 0.7431\n","Epoch: 005, Train Acc: 0.8030, Validation Acc: 0.8002\n","Epoch: 006, Train Acc: 0.8017, Validation Acc: 0.8021\n","Epoch: 007, Train Acc: 0.8574, Validation Acc: 0.8516\n","Epoch: 008, Train Acc: 0.8830, Validation Acc: 0.8677\n","Epoch: 009, Train Acc: 0.8620, Validation Acc: 0.8592\n","Epoch: 010, Train Acc: 0.8793, Validation Acc: 0.8839\n","Epoch: 011, Train Acc: 0.8926, Validation Acc: 0.8887\n","Epoch: 012, Train Acc: 0.9013, Validation Acc: 0.8991\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","numClasses = len(list(unique_classes))\n","\n","\n","\n","\n","model = GCN(hidden_channels=64, numClasses=numClasses, numFeatures=33).to(device=device)\n","print(model)\n","\n","\n","\n","\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-5)\n","penaltyWeights = torch.tensor( Class_Weights)  # <--- THOSE WEIGHTS ARE COMPUTED MANUALLY ACCORDING TO AN EQUATION\n","criterion = torch.nn.CrossEntropyLoss(weight=penaltyWeights.to(device=device))\n","\n","\n","\n","\n","\n","\n","for epoch in range(1, 13):\n","    train(model, loader, device, optimizer, criterion)\n","    train_acc = test(model, loader, device)\n","    val_acc = test(model, val_loader, device)\n","    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Validation Acc: {val_acc:.4f}')\n","\n","\n","\n","\n","\n","# Saving the model\n","with open('GCN.pkl', 'wb') as f:\n","    pickle.dump(model.cpu(), f)\n","\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00      1188\n","           1       0.83      0.92      0.87       767\n","           2       0.83      0.93      0.88       481\n","           3       0.46      0.11      0.18       190\n","\n","    accuracy                           0.90      2626\n","   macro avg       0.78      0.74      0.73      2626\n","weighted avg       0.88      0.90      0.88      2626\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device=device)\n","\n","y_true = []\n","y_predicted = []\n","unique_classes = set()\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    \n","    pred,_ = inference(model, d.to(device=device), device=device)\n","    y_predicted.append(pred)\n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### You input 2 folders, one folder containing the json of nodes, the other contains the csv for edges.<br>don't mix them in one folder, bad things happen<br>if you have a npy file, please pass its abs path as this file will make the script run much faster"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n","52\n","53\n","54\n","55\n","56\n","57\n","58\n","59\n","60\n","61\n","62\n","63\n","64\n","65\n","66\n","67\n","68\n","69\n","70\n","71\n","72\n","73\n","74\n","75\n","76\n","77\n","78\n","79\n","80\n","81\n","82\n","83\n","84\n","85\n","86\n","87\n","88\n","89\n","90\n","91\n","92\n","93\n","94\n","95\n","96\n","97\n","98\n","99\n","100\n","101\n","102\n","103\n","104\n","105\n","106\n","107\n","108\n","109\n","110\n","111\n","112\n","113\n","114\n","115\n","116\n","117\n","118\n","119\n","120\n","121\n","122\n","123\n","124\n","125\n","126\n","127\n","128\n","129\n","130\n","131\n","132\n","133\n","134\n","135\n","136\n","137\n","138\n","139\n","140\n","141\n","142\n","143\n","144\n","145\n","146\n","147\n","148\n","149\n","150\n","151\n","152\n","153\n","154\n","155\n","156\n","157\n","158\n","159\n","160\n"]}],"source":["import numpy as np\n","\n","pathToUser_Nodes = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\\nodes_457_test\".replace('\\\\', '/')\n","pathToUser_Edges = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\\edges_457_test\".replace('\\\\', '/')\n","outPath = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\".replace('\\\\', '/')\n","classification, embeddings = InferenceGCN(pathToUser_Nodes, pathToUser_Edges, outPath, multipleFiles='true', npyPath=None)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","------------------------------------------------\n","[-0.01425866  0.00964587 -0.0248956  -0.05527318  0.00654169  0.00597529\n","  0.04964018  0.07472066  0.2547075   0.05761394 -0.07598666  0.04683467\n"," -0.1981306  -0.01381772 -0.00594068  0.13489778  0.05207491 -0.04593862\n"," -0.06843887 -0.17021583  0.02443486 -0.03699596 -0.12537734 -0.01297841\n"," -0.03448276 -0.03025983  0.10159049 -0.05370921 -0.02003104  0.03488482\n","  0.24602173 -0.05476311 -0.2632475  -0.03785818  0.16886538  0.07249996\n","  0.05839108 -0.06247886  0.04957579  0.01944224  0.070255    0.02622986\n","  0.05911631 -0.08293047 -0.08957326  0.12523521 -0.02537146 -0.03732213\n","  0.21424401  0.00181731  0.01555901 -0.0749732  -0.02018991 -0.0464328\n"," -0.19937828  0.03486762 -0.18280306 -0.01232024  0.07163822 -0.11567902\n","  0.09055584 -0.1092463   0.18654798  0.08254305]\n","------------------------------------------------\n"]}],"source":["print(classification[0])\n","print('------------------------------------------------')\n","print(embeddings[0])\n","print('------------------------------------------------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Reading the outputted CSV"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>56</th>\n","      <th>57</th>\n","      <th>58</th>\n","      <th>59</th>\n","      <th>60</th>\n","      <th>61</th>\n","      <th>62</th>\n","      <th>63</th>\n","      <th>64</th>\n","      <th>65</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>CWE457_Use_of_Uninitialized_Variable__char_poi...</td>\n","      <td>-0.014259</td>\n","      <td>0.009646</td>\n","      <td>-0.024896</td>\n","      <td>-0.055273</td>\n","      <td>0.006542</td>\n","      <td>0.005975</td>\n","      <td>0.049640</td>\n","      <td>0.074721</td>\n","      <td>0.254707</td>\n","      <td>...</td>\n","      <td>0.034868</td>\n","      <td>-0.182803</td>\n","      <td>-0.012320</td>\n","      <td>0.071638</td>\n","      <td>-0.115679</td>\n","      <td>0.090556</td>\n","      <td>-0.109246</td>\n","      <td>0.186548</td>\n","      <td>0.082543</td>\n","      <td>CWE457</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>CWE457_Use_of_Uninitialized_Variable__char_poi...</td>\n","      <td>-0.041305</td>\n","      <td>0.023144</td>\n","      <td>-0.039340</td>\n","      <td>-0.073968</td>\n","      <td>-0.017634</td>\n","      <td>-0.016639</td>\n","      <td>0.057879</td>\n","      <td>0.096288</td>\n","      <td>0.250582</td>\n","      <td>...</td>\n","      <td>0.014195</td>\n","      <td>-0.174585</td>\n","      <td>-0.034068</td>\n","      <td>0.086569</td>\n","      <td>-0.122454</td>\n","      <td>0.103462</td>\n","      <td>-0.094001</td>\n","      <td>0.190281</td>\n","      <td>0.103685</td>\n","      <td>CWE457</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>CWE457_Use_of_Uninitialized_Variable__char_poi...</td>\n","      <td>-0.026269</td>\n","      <td>0.013434</td>\n","      <td>-0.032160</td>\n","      <td>-0.061153</td>\n","      <td>-0.002854</td>\n","      <td>-0.003262</td>\n","      <td>0.053972</td>\n","      <td>0.083537</td>\n","      <td>0.255258</td>\n","      <td>...</td>\n","      <td>0.026661</td>\n","      <td>-0.181065</td>\n","      <td>-0.020981</td>\n","      <td>0.076601</td>\n","      <td>-0.119521</td>\n","      <td>0.096819</td>\n","      <td>-0.104968</td>\n","      <td>0.189877</td>\n","      <td>0.092368</td>\n","      <td>CWE457</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>CWE457_Use_of_Uninitialized_Variable__double_0...</td>\n","      <td>-0.064210</td>\n","      <td>0.033937</td>\n","      <td>-0.048949</td>\n","      <td>-0.092147</td>\n","      <td>-0.039621</td>\n","      <td>-0.039377</td>\n","      <td>0.065252</td>\n","      <td>0.116362</td>\n","      <td>0.250451</td>\n","      <td>...</td>\n","      <td>-0.003604</td>\n","      <td>-0.172618</td>\n","      <td>-0.054271</td>\n","      <td>0.099797</td>\n","      <td>-0.127058</td>\n","      <td>0.118339</td>\n","      <td>-0.085054</td>\n","      <td>0.197350</td>\n","      <td>0.123417</td>\n","      <td>CWE457</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>CWE457_Use_of_Uninitialized_Variable__double_0...</td>\n","      <td>-0.039051</td>\n","      <td>0.022689</td>\n","      <td>-0.038575</td>\n","      <td>-0.072574</td>\n","      <td>-0.015784</td>\n","      <td>-0.014410</td>\n","      <td>0.057055</td>\n","      <td>0.094325</td>\n","      <td>0.249551</td>\n","      <td>...</td>\n","      <td>0.015565</td>\n","      <td>-0.173711</td>\n","      <td>-0.032124</td>\n","      <td>0.085513</td>\n","      <td>-0.121968</td>\n","      <td>0.101585</td>\n","      <td>-0.093938</td>\n","      <td>0.188728</td>\n","      <td>0.101600</td>\n","      <td>CWE457</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows Ã— 66 columns</p>\n","</div>"],"text/plain":["                                                  0         1         2   \\\n","0  CWE457_Use_of_Uninitialized_Variable__char_poi... -0.014259  0.009646   \n","1  CWE457_Use_of_Uninitialized_Variable__char_poi... -0.041305  0.023144   \n","2  CWE457_Use_of_Uninitialized_Variable__char_poi... -0.026269  0.013434   \n","3  CWE457_Use_of_Uninitialized_Variable__double_0... -0.064210  0.033937   \n","4  CWE457_Use_of_Uninitialized_Variable__double_0... -0.039051  0.022689   \n","\n","         3         4         5         6         7         8         9   ...  \\\n","0 -0.024896 -0.055273  0.006542  0.005975  0.049640  0.074721  0.254707  ...   \n","1 -0.039340 -0.073968 -0.017634 -0.016639  0.057879  0.096288  0.250582  ...   \n","2 -0.032160 -0.061153 -0.002854 -0.003262  0.053972  0.083537  0.255258  ...   \n","3 -0.048949 -0.092147 -0.039621 -0.039377  0.065252  0.116362  0.250451  ...   \n","4 -0.038575 -0.072574 -0.015784 -0.014410  0.057055  0.094325  0.249551  ...   \n","\n","         56        57        58        59        60        61        62  \\\n","0  0.034868 -0.182803 -0.012320  0.071638 -0.115679  0.090556 -0.109246   \n","1  0.014195 -0.174585 -0.034068  0.086569 -0.122454  0.103462 -0.094001   \n","2  0.026661 -0.181065 -0.020981  0.076601 -0.119521  0.096819 -0.104968   \n","3 -0.003604 -0.172618 -0.054271  0.099797 -0.127058  0.118339 -0.085054   \n","4  0.015565 -0.173711 -0.032124  0.085513 -0.121968  0.101585 -0.093938   \n","\n","         63        64      65  \n","0  0.186548  0.082543  CWE457  \n","1  0.190281  0.103685  CWE457  \n","2  0.189877  0.092368  CWE457  \n","3  0.197350  0.123417  CWE457  \n","4  0.188728  0.101600  CWE457  \n","\n","[5 rows x 66 columns]"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","newdf = pd.read_csv(f\"{outPath}/embeddings.csv\".replace('\\\\', '/'), header=None, index_col=None)\n","newdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
