{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Graph Classification"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch, Dataset\n","from torch_geometric.nn import GCNConv, Sequential, Linear, global_mean_pool\n","import pickle\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n","import torch_geometric.utils as ut\n","from sklearn.model_selection import train_test_split\n","\n","from gcnHelpers import *\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training on Multiple classes\n","#### please put the folders of the CVE's in the script's directory<br>then inside each folder put 2 subfolders, one for the nodes and the other for edges\n","##### example: folder 121 contains 4 subfolders (nodes_121, edges_121, feature_matrices, nodes_targets)<br>and feature_matrices and nodes_targets contain the npy files"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# 121   122     ok\n","# 23    36      not ok\n","# 126   127     not ok\n","# 194   195     not ok"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 23------------------\n","num Graphs : 3313\n","num nodes : 8855827 of which 37034 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 3313\n","number of files to be trained on = 3313\n","\n","\n","-----------LOADING CVE 126------------------\n","num Graphs : 1949\n","num nodes : 4179998 of which 9807 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 1949\n","number of files to be trained on = 1949\n","\n","\n","-----------LOADING CVE 194------------------\n","num Graphs : 1592\n","num nodes : 3381463 of which 16192 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 1592\n","number of files to be trained on = 1592\n","\n","\n","-----------LOADING CVE 401------------------\n","num Graphs : 2261\n","num nodes : 4553383 of which 8766 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 2261\n","number of files to be trained on = 2261\n","\n","\n","-----------LOADING CVE 690------------------\n","num Graphs : 1326\n","num nodes : 2641364 of which 3584 are vulnerable\n","number of corrupted files due to missing node features for certain nodes = 0\n","total number of files 1326\n","number of files to be trained on = 1326\n","\n","\n"]}],"source":["#CVES_to_train_on = {'762':4, '191':5, '134':6, '590':7}#, '23', '401', '457', '590', '690', '762', '78', '23', '36', '126', '127', '194', '195', '191']\n","#CVES_to_train_on = ['122', '121', '78', '190']\n","#CVES_to_train_on = ['762', '191', '134', '590']\n","#CVES_to_train_on = ['23', '126', '194', '401','690']\n","CVES_to_train_on = ['36', '127', '124', '195', '457']\n","\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","numGraphs_List = []\n","\n","torch.cuda.empty_cache()\n","\n","for encClass, cve in enumerate(CVES_to_train_on):\n","    print(f\"-----------LOADING CVE {cve}------------------\")\n","\n","    \n","    pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}/edges_{cve}\"\n","\n","    with open (f'{cve}/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","        features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","    with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","        nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","    \n","    print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","    numGraphs_List.append(features_matrices_list.shape[0])\n","\n","\n","    # Reading the adjacency list of each graph\n","    adj_Lists = []\n","    for i, filename in enumerate(os.listdir(pathToEdges)):\n","       \n","        try:\n","            f = os.path.join(pathToEdges, filename)\n","            df = pd.read_csv(f, header=None)\n","            df.dropna(inplace=True)             # <---- here is the dropping\n","            adj_Lists.append(df)\n","        except:\n","            print('something wrong')\n","        \n","\n","\n","    numVulnNodes = 0\n","\n","    encoded = encClass\n","    # Reading the label of each node\n","    Targets_List = []\n","    for i, target in enumerate(nodes_targets_list):\n","        if(i == len(adj_Lists)):\n","            break\n","        numVulnNodes += sum(target)\n","        Targets_List.append(encoded)\n","            \n","\n","    numNodes = 0\n","    # Reading the features of each node\n","    node_Features_List = []\n","    \n","    for i, node_feature in enumerate(features_matrices_list):\n","        n = np.array(node_feature,dtype='int64')\n","        numNodes+= n.shape[0]\n","        node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","    try:\n","        assert (len(adj_Lists) == len(Targets_List))\n","        assert (len(adj_Lists) == len(node_Features_List))\n","    except:\n","        print(len(adj_Lists))\n","        print(len(Targets_List))\n","        print(len(node_Features_List))\n","\n","\n","    # Convert the edge list to use 0-based indices\n","    adj_Tensors = []\n","    for adj in adj_Lists:\n","        # the documentation strictly says to convert the adjacency list to a contiguous list\n","        adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","    #Iterate over each graph, make a Data object, then append to all our dataset\n","    cntCorruptData=0\n","    for i, adj in enumerate(adj_Tensors):\n","        d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor(Targets_List[i]))  # <--- from the documentation.\n","        try:\n","            d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","        except:\n","            print('DIMENSION ERROR')\n","            print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","            print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","            cntCorruptData +=1\n","            continue\n","        allData.append(d)\n","\n","\n","    print(f\"num nodes : {numNodes} of which {numVulnNodes} are vulnerable\")\n","    print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","    print(f\"total number of files {len(adj_Tensors)}\")\n","    print(f\"number of files to be trained on = {len(adj_Tensors) - cntCorruptData}\\n\\n\")\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Computing class weights to be used in a weighted loss function"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Weights for each class are:\n"," [3.1515242982191367, 5.357106208311955, 6.558417085427136, 4.617868199911544, 7.874057315233785]\n"]}],"source":["total_Graphs = sum(numGraphs_List)\n","Class_Weights = [total_Graphs/i for i in numGraphs_List]\n","print(f\"Weights for each class are:\\n {Class_Weights}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train-Val-Test-Split and data loader"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes {0, 1, 2, 3, 4}\n"]}],"source":["\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.2, shuffle=True)\n","\n","# Train Validation Split\n","allData_train, allData_val = train_test_split(allData_train, test_size=0.1, shuffle=True)\n","\n","unique_classes = set()\n","\n","for d in allData_train:\n","    unique_classes.add(int(d.y.numpy()))\n","\n","print(f\"Unique classes {unique_classes}\")\n","\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(allData_test, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(allData_val, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class\n","#### anything related to the architecture"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:49:10.223288Z","iopub.status.busy":"2023-06-26T10:49:10.222397Z","iopub.status.idle":"2023-06-26T10:49:31.983085Z","shell.execute_reply":"2023-06-26T10:49:31.981783Z","shell.execute_reply.started":"2023-06-26T10:49:10.223243Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(33, 128)\n","  (conv2): GCNConv(128, 128)\n","  (conv3): GCNConv(128, 128)\n","  (conv4): GCNConv(128, 128)\n","  (conv5): GCNConv(128, 128)\n","  (fc2): Linear(in_features=128, out_features=128, bias=True)\n","  (lin): Linear(in_features=128, out_features=5, bias=True)\n",")\n","Epoch: 001, Train Acc: 0.3655, Validation Acc: 0.3708\n","Epoch: 002, Train Acc: 0.3857, Validation Acc: 0.3888\n","Epoch: 003, Train Acc: 0.4722, Validation Acc: 0.4450\n","Epoch: 004, Train Acc: 0.6034, Validation Acc: 0.5778\n","Epoch: 005, Train Acc: 0.7098, Validation Acc: 0.7273\n","Epoch: 006, Train Acc: 0.6486, Validation Acc: 0.6124\n","Epoch: 007, Train Acc: 0.8489, Validation Acc: 0.8732\n","Epoch: 008, Train Acc: 0.7171, Validation Acc: 0.7129\n","Epoch: 009, Train Acc: 0.9625, Validation Acc: 0.9653\n","Epoch: 010, Train Acc: 0.9629, Validation Acc: 0.9629\n","Epoch: 011, Train Acc: 0.9730, Validation Acc: 0.9797\n","Epoch: 012, Train Acc: 0.9557, Validation Acc: 0.9641\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","numClasses = len(list(unique_classes))\n","\n","model = GCN(hidden_channels=128, numClasses=numClasses, numFeatures=33).to(device=device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-7)\n","penaltyWeights = torch.tensor( Class_Weights)  # <--- THOSE WEIGHTS ARE COMPUTED MANUALLY ACCORDING TO AN EQUATION\n","criterion = torch.nn.CrossEntropyLoss(weight=penaltyWeights.to(device=device))\n","\n","for epoch in range(1, 13):\n","    train(model, loader, device, optimizer, criterion)\n","    train_acc = test(model, loader, device)\n","    val_acc = test(model, val_loader, device)\n","    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Validation Acc: {val_acc:.4f}')\n","\n","# Saving the model\n","with open('GCN_class_g4_2.pkl', 'wb') as f:\n","    pickle.dump(model.cpu(), f)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.97      1.00      0.99       645\n","           1       0.99      0.90      0.94       394\n","           2       0.99      0.95      0.97       299\n","           3       0.95      0.94      0.95       469\n","           4       0.90      1.00      0.95       282\n","\n","    accuracy                           0.96      2089\n","   macro avg       0.96      0.96      0.96      2089\n","weighted avg       0.96      0.96      0.96      2089\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","with open('GCN_class_g4_2.pkl', 'rb') as f:\n","    model = pickle.load(f)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device=device)\n","\n","y_true = []\n","y_predicted = []\n","unique_classes = set()\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    \n","    pred,_ = inference(model, d.to(device=device), device=device)\n","    y_predicted.append(pred)\n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Inference on only 1 datapoint"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### You input 2 folders, one folder containing the json of nodes, the other contains the csv for edges.<br>don't mix them in one folder, bad things happen<br>if you have a npy file, please pass its abs path as this file will make the script run much faster"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"InferenceGCN() got multiple values for argument 'multipleFiles'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[8], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m pathToUser_Edges \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mClassWork\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGuardista\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m4-Features_Extraction\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m457_test\u001b[39m\u001b[39m\\\u001b[39m\u001b[39medges_457_test\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m outPath \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mClassWork\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mGuardista\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m4-Features_Extraction\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m457_test\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\\\\u001b[39;00m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m classification, Embdding_df \u001b[39m=\u001b[39m InferenceGCN(\u001b[39m'\u001b[39;49m\u001b[39mGCN.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m,pathToUser_Nodes, pathToUser_Edges, outPath, multipleFiles\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrue\u001b[39;49m\u001b[39m'\u001b[39;49m, npyPath\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n","\u001b[1;31mTypeError\u001b[0m: InferenceGCN() got multiple values for argument 'multipleFiles'"]}],"source":["import numpy as np\n","\n","pathToUser_Nodes = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\\nodes_457_test\".replace('\\\\', '/')\n","pathToUser_Edges = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\\edges_457_test\".replace('\\\\', '/')\n","outPath = r\"D:\\ClassWork\\Guardista\\4-Features_Extraction\\test\\457_test\".replace('\\\\', '/')\n","classification, Embdding_df = InferenceGCN('GCN.pkl',pathToUser_Nodes, pathToUser_Edges, outPath, multipleFiles='true', npyPath=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","\u001b[1;31mUnable to start Kernel 'venv (Python 3.8.0)' due to a timeout waiting for the ports to get used. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["print(classification[0])\n","print('------------------------------------------------')\n","print(Embdding_df.head())\n","print('------------------------------------------------')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Reading the outputted CSV"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"","evalue":"","output_type":"error","traceback":["\u001b[1;31mFailed to start the Kernel. \n","\u001b[1;31mUnable to start Kernel 'venv (Python 3.8.0)' due to a timeout waiting for the ports to get used. \n","\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."]}],"source":["import pandas as pd\n","newdf = pd.read_csv(f\"{outPath}/embeddings.csv\".replace('\\\\', '/'), header=None, index_col=None)\n","newdf.head()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files discarded = 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6627/6627 [01:09<00:00, 95.32it/s] \n"]}],"source":["\n","\n","cve = '78'\n","outPath = 'D:/ClassWork/Guardista/4-Features_Extraction/78'.replace('\\\\', '/')\n","gcnModelPath = \"D:/ClassWork/Guardista/4-Features_Extraction/GCN_class_g1\".replace('\\\\','/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/78/edges_78\".replace('\\\\', '/')\n","\n","\n","npyPath = outPath+'/features_matrices/features_matrices_' + cve + '.npy'\n","\n","\n","classification_1, embeddings_df_1 = InferenceGCN( gcnModelPath , pathToUser_Edges=pathToUser_Edges , outputPath=outPath, multipleFiles='true', npyPath=npyPath, cve=cve)\n","#embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_1.to_csv(f\"{outPath}/embeddings_{cve}.csv\", header=False, index=False)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files discarded = 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6627/6627 [01:02<00:00, 106.43it/s]\n"]}],"source":["cve = '78'\n","outPath = 'D:/ClassWork/Guardista/4-Features_Extraction/78'.replace('\\\\', '/')\n","gcnModelPath = \"D:/ClassWork/Guardista/4-Features_Extraction/GCN_class_g1_safe\".replace('\\\\','/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/78/edges_78\".replace('\\\\', '/')\n","\n","\n","npyPath = outPath+'/features_matrices/features_matrices_' + cve + '.npy'\n","\n","\n","classification_2, embeddings_df_2 = InferenceGCN( gcnModelPath , pathToUser_Edges=pathToUser_Edges , outputPath=outPath, multipleFiles='true', npyPath=npyPath, cve=cve)\n","#embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_2.to_csv(f\"{outPath}/embeddings_{cve}.csv\", header=False, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["258\n"]}],"source":["LabelColumn = embeddings_df_1.iloc[:, -1]\n","embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_2.drop(columns=(embeddings_df_2.columns)[0], inplace=True)\n","embeddings_df_2.drop(columns=(embeddings_df_2.columns)[-1], inplace=True)\n","finalDF = pd.concat([embeddings_df_1, embeddings_df_2, LabelColumn], axis=1, ignore_index=True)\n","print(len(finalDF.columns))\n","finalDF.to_csv(f\"{outPath}/concatEmbeddings_{cve}.csv\", header=False, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files discarded = 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6630/6630 [01:07<00:00, 98.28it/s] \n"]}],"source":["\n","cve = '78_safe'\n","outPath = 'D:/ClassWork/Guardista/4-Features_Extraction/78_safe'.replace('\\\\', '/')\n","gcnModelPath = \"D:/ClassWork/Guardista/4-Features_Extraction/GCN_class_g1.pkl\".replace('\\\\','/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/78_safe/edges_78_safe\".replace('\\\\', '/')\n","\n","\n","npyPath = outPath+'/features_matrices/features_matrices_' + cve + '.npy'\n","\n","\n","classification_1, embeddings_df_1 = InferenceGCN( gcnModelPath , pathToUser_Edges=pathToUser_Edges , outputPath=outPath, multipleFiles='true', npyPath=npyPath, cve=cve)\n","#embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_1.to_csv(f\"{outPath}/embeddings_{cve}.csv\", header=False, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files discarded = 0\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 6630/6630 [01:07<00:00, 98.89it/s] \n"]}],"source":["cve = '78_safe'\n","outPath = 'D:/ClassWork/Guardista/4-Features_Extraction/78_safe'.replace('\\\\', '/')\n","gcnModelPath = \"D:/ClassWork/Guardista/4-Features_Extraction/GCN_class_g1_safe.pkl\".replace('\\\\','/')\n","pathToUser_Edges = \"D:/ClassWork/Guardista/4-Features_Extraction/78_safe/edges_78_safe\".replace('\\\\', '/')\n","\n","\n","npyPath = outPath+'/features_matrices/features_matrices_' + cve + '.npy'\n","\n","\n","classification_2, embeddings_df_2 = InferenceGCN( gcnModelPath , pathToUser_Edges=pathToUser_Edges , outputPath=outPath, multipleFiles='true', npyPath=npyPath, cve=cve)\n","#embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_2.to_csv(f\"{outPath}/embeddings_{cve}.csv\", header=False, index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["258\n"]}],"source":["LabelColumn = embeddings_df_1.iloc[:, -1]\n","embeddings_df_1.drop(columns=list(embeddings_df_1.columns)[-1], inplace=True)\n","embeddings_df_2.drop(columns=(embeddings_df_2.columns)[0], inplace=True)\n","embeddings_df_2.drop(columns=(embeddings_df_2.columns)[-1], inplace=True)\n","finalDF = pd.concat([embeddings_df_1, embeddings_df_2, LabelColumn], axis=1, ignore_index=True)\n","print(len(finalDF.columns))\n","finalDF.to_csv(f\"{outPath}/concatEmbeddings_{cve}.csv\", header=False, index=False)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
