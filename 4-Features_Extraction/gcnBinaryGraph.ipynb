{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Binary Graph Classification\n","#### Done on the smallest class 457<br>using the vulnerable and safe file of this class only"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, Sequential, Linear, global_mean_pool\n","import pickle\n","from torch_geometric.data import Dataset\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","#from gcnHelpersGraph import GCN, preProcessingOneDataPoint, train, test"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training on Binary class\n","#### please put the folders of the CVE's in the script's directory<br>then inside each folder put 2 subfolders, one for the nodes and the other for edges\n","##### also for the safe class, name the folder 457_SAFE, and inside it nodes_457_SAFE and edges_457_SAFE"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 457 Vulnerable------------------\n","num Graphs : 914\n","number of corrupted files due to missing node features for certain nodes = 0\n"]}],"source":["\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","\n","torch.cuda.empty_cache()\n","\n","cve = '457'\n","print(f\"-----------LOADING CVE {cve} Vulnerable------------------\")\n","\n","\n","pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}/edges_{cve}\"\n","\n","with open (f'{cve}/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","    nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","\n","print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for filename in os.listdir(pathToEdges):\n","    f = os.path.join(pathToEdges, filename)\n","    df = pd.read_csv(f, header=None)\n","    df.dropna(inplace=True)             # <---- here is the dropping\n","    adj_Lists.append(df)\n","    \n","\n","\n","numVulnNodes = 0\n","\n","\n","# Reading the label of each node\n","Targets_List = []\n","for target in nodes_targets_list:\n","    numVulnNodes += sum(target)\n","    Targets_List.append(1)\n","    \n","\n","numNodes = 0\n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    numNodes+= n.shape[0]\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","assert (len(adj_Lists) == len(Targets_List))\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor([1]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        print('DIMENSION ERROR')\n","        print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Loading the 457 SAFE dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 457 SAFE------------------\n","num Graphs : 913\n","number of corrupted files due to missing node features for certain nodes = 0\n"]}],"source":["print(f\"-----------LOADING CVE {cve} SAFE------------------\")\n","\n","\n","pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}_SAFE/edges_{cve}_SAFE\"\n","\n","with open (f'{cve}_SAFE/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","with open(f'{cve}_SAFE/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","    nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","\n","print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for filename in os.listdir(pathToEdges):\n","    f = os.path.join(pathToEdges, filename)\n","    df = pd.read_csv(f, header=None)\n","    df.dropna(inplace=True)             # <---- here is the dropping\n","    adj_Lists.append(df)\n","    \n","\n","\n","numVulnNodes = 0\n","\n","\n","# Reading the label of each node\n","Targets_List = []\n","for target in nodes_targets_list:\n","    numVulnNodes += sum(target)\n","    Targets_List.append(0)\n","    \n","\n","numNodes = 0\n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    numNodes+= n.shape[0]\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","assert (len(adj_Lists) == len(Targets_List))\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor([0]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        print('DIMENSION ERROR')\n","        print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train-Test-Split and data loader"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes {0, 1}\n"]}],"source":["\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.3, shuffle=True)\n","\n","\n","unique_classes = set()\n","\n","for d in allData_train:\n","    unique_classes.add(int(d.y.numpy()))\n","\n","print(f\"Unique classes {unique_classes}\")\n","\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(allData_test, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:49:10.223288Z","iopub.status.busy":"2023-06-26T10:49:10.222397Z","iopub.status.idle":"2023-06-26T10:49:31.983085Z","shell.execute_reply":"2023-06-26T10:49:31.981783Z","shell.execute_reply.started":"2023-06-26T10:49:10.223243Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(33, 64)\n","  (conv2): GCNConv(64, 64)\n","  (conv3): GCNConv(64, 64)\n","  (lin): Linear(64, 2, bias=True)\n",")\n","Epoch: 001, Train Acc: 0.5016\n","Epoch: 002, Train Acc: 0.4984\n","Epoch: 003, Train Acc: 0.5016\n","Epoch: 004, Train Acc: 0.4984\n","Epoch: 005, Train Acc: 0.5016\n","Epoch: 006, Train Acc: 0.4984\n","Epoch: 007, Train Acc: 0.4984\n","Epoch: 008, Train Acc: 0.4984\n","Epoch: 009, Train Acc: 0.4984\n","Epoch: 010, Train Acc: 0.5016\n","Epoch: 011, Train Acc: 0.4984\n","Epoch: 012, Train Acc: 0.5016\n","Epoch: 013, Train Acc: 0.5016\n","Epoch: 014, Train Acc: 0.5016\n","Epoch: 015, Train Acc: 0.4984\n","Epoch: 016, Train Acc: 0.5016\n","Epoch: 017, Train Acc: 0.5016\n","Epoch: 018, Train Acc: 0.5016\n","Epoch: 019, Train Acc: 0.4984\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","numClasses = 2\n","\n","\n","\n","class GCN(torch.nn.Module):\n","    def __init__(self, hidden_channels):\n","        super(GCN, self).__init__()\n","        torch.manual_seed(12345)\n","        self.conv1 = GCNConv(33, hidden_channels)\n","        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n","        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n","        self.lin = Linear(hidden_channels, numClasses)\n","\n","    def forward(self, x, edge_index, batch):\n","        # 1. Obtain node embeddings \n","        x = self.conv1(x, edge_index)\n","        x = x.relu()\n","        x = self.conv2(x, edge_index)\n","        x = x.relu()\n","        x = self.conv3(x, edge_index)\n","\n","        # 2. Readout layer\n","        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n","\n","        # 3. Apply a final classifier\n","        x = F.dropout(x, p=0.5, training=self.training)\n","        x = self.lin(x)\n","        \n","        return x\n","\n","model = GCN(hidden_channels=64).to(device=device)\n","print(model)\n","\n","\n","\n","\n","\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-5)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","\n","\n","\n","\n","def train():\n","    model.train()\n","\n","    for data in loader:  # Iterate in batches over the training dataset.\n","         out = model(data.x.to(device=device), data.edge_index.to(device=device), data.batch.to(device=device))  # Perform a single forward pass.\n","         loss = criterion(out, data.y.to(device=device))  # Compute the loss.\n","         loss.backward()  # Derive gradients.\n","         optimizer.step()  # Update parameters based on gradients.\n","         optimizer.zero_grad()  # Clear gradients.\n","\n","\n","\n","def test(loader):\n","     model.eval()\n","\n","     correct = 0\n","     for data in loader:  # Iterate in batches over the training/test dataset.\n","         out = model(data.x.to(device=device), data.edge_index.to(device=device), data.batch.to(device=device))  \n","         pred = out.argmax(dim=1)  # Use the class with highest probability.\n","         correct += int((pred == data.y.to(device=device)).sum())  # Check against ground-truth labels.\n","     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n","\n","def inference(model, data_point):\n","    model.eval().to(device=device)\n","    with torch.no_grad():\n","        x = data_point.x.to(device=device)\n","        edge_index = data_point.edge_index.to(device=device)\n","        batch = data_point.batch\n","        out = model(x, edge_index, batch)\n","        pred = out.argmax(dim=1).cpu().item()\n","    return pred\n","\n","\n","for epoch in range(1, 20):\n","    train()\n","    train_acc = test(loader)\n","    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}')\n","\n","\n","\n","\n","\n","# Saving the model\n","#with open('GCN.pkl', 'wb') as f:\n","#    pickle.dump(model.cpu(), f)\n","\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.50      1.00      0.67       276\n","           1       0.00      0.00      0.00       273\n","\n","    accuracy                           0.50       549\n","   macro avg       0.25      0.50      0.33       549\n","weighted avg       0.25      0.50      0.34       549\n","\n"]},{"name":"stderr","output_type":"stream","text":["d:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","d:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","d:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device=device)\n","\n","y_true = []\n","y_predicted = []\n","unique_classes = set()\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    \n","    pred = inference(model, d.to(device=device))\n","    y_predicted.append(pred)\n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
