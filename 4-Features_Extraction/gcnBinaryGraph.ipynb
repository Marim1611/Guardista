{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Binary Graph Classification\n","#### Done on the smallest class 457<br>using the vulnerable and safe file of this class only"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2023-06-26T10:37:45.233769Z","iopub.status.busy":"2023-06-26T10:37:45.233419Z","iopub.status.idle":"2023-06-26T10:37:45.298213Z","shell.execute_reply":"2023-06-26T10:37:45.297254Z","shell.execute_reply.started":"2023-06-26T10:37:45.233742Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import json\n","import torch\n","import os\n","from torch_geometric.data import Data, Batch\n","from torch_geometric.nn import GCNConv, Sequential, Linear, global_mean_pool\n","import pickle\n","from torch_geometric.data import Dataset\n","from torch_geometric.loader import DataLoader\n","import torch.nn.functional as F\n","\n","\n","\n","\n","from sklearn.model_selection import train_test_split\n","\n","from gcnHelpers import *"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Training on Binary class\n","#### please put the folders of the CVE's in the script's directory<br>then inside each folder put 2 subfolders, one for the nodes and the other for edges\n","##### also for the safe class, name the folder 457_SAFE, and inside it nodes_457_SAFE and edges_457_SAFE"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 78 Vulnerable------------------\n","1500\n","num Graphs : 1500\n","1500\n","number of corrupted files due to missing node features for certain nodes = 622\n"]}],"source":["\n","#List containing all input data of type Data (Pytorch geometric datastructure that holds a single graph)\n","allData = []\n","\n","torch.cuda.empty_cache()\n","\n","cve = '78'\n","print(f\"-----------LOADING CVE {cve} Vulnerable------------------\")\n","\n","\n","pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}/edges_{cve}\"\n","\n","with open (f'{cve}/features_matrices/features_matrices_'+cve+'.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","stop_after = len(features_matrices_list)\n","print(stop_after)\n","\n","# with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","#     nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","\n","print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for i, filename in enumerate(os.listdir(pathToEdges)):\n","    if i == stop_after:\n","        break\n","    try:\n","        f = os.path.join(pathToEdges, filename)\n","        df = pd.read_csv(f, header=None)\n","        df.dropna(inplace=True)             # <---- here is the dropping\n","        adj_Lists.append(df)\n","    except:\n","        print('something wrong')\n","\n","        \n","    \n","print(len(adj_Lists))\n","\n","numVulnNodes = 0\n","\n","\n","    \n","\n","numNodes = 0\n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    numNodes+= n.shape[0]\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor([1]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        #print('DIMENSION ERROR')\n","        #print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        #print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")\n","   "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Loading the 457 SAFE dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-----------LOADING CVE 78 SAFE------------------\n","1500\n","num Graphs : 1500\n","1500\n","number of corrupted files due to missing node features for certain nodes = 0\n"]}],"source":["print(f\"-----------LOADING CVE {cve} SAFE------------------\")\n","\n","pathToEdges = f\"D:/ClassWork/Guardista/4-Features_Extraction/{cve}_safe/edges_{cve}_safe\"\n","\n","with open (f'{cve}_safe/features_matrices/features_matrices_'+cve+'_safe.npy', 'rb') as f:\n","    features_matrices_list = np.load(f,  allow_pickle=True)\n","\n","stop_after = len(features_matrices_list)\n","print(stop_after)\n","\n","# with open(f'{cve}/nodes_targets/nodes_targets_'+cve+'.npy', 'rb') as f:\n","#     nodes_targets_list = np.load(f,  allow_pickle=True)\n","\n","\n","print(f\"num Graphs : {features_matrices_list.shape[0]}\")\n","\n","\n","# Reading the adjacency list of each graph\n","adj_Lists = []\n","for i, filename in enumerate(os.listdir(pathToEdges)):\n","    if i == stop_after:\n","        break\n","    try:\n","        f = os.path.join(pathToEdges, filename)\n","        df = pd.read_csv(f, header=None)\n","        df.dropna(inplace=True)             # <---- here is the dropping\n","        adj_Lists.append(df)\n","    except:\n","        print('something wrong')\n","\n","        \n","    \n","print(len(adj_Lists))\n","\n","numVulnNodes = 0\n","\n","\n","# Reading the label of each node\n","#Targets_List = []\n","\n","# for target in nodes_targets_list:\n","#     numVulnNodes += sum(target)\n","#     Targets_List.append(1)\n","\n","    \n","\n","numNodes = 0\n","# Reading the features of each node\n","node_Features_List = []\n","for node_feature in features_matrices_list:\n","    n = np.array(node_feature,dtype='int64')\n","    numNodes+= n.shape[0]\n","    node_Features_List.append(torch.tensor(n, dtype=torch.float))\n","\n","\n","assert (len(adj_Lists) == len(node_Features_List))\n","\n","\n","# Convert the edge list to use 0-based indices\n","adj_Tensors = []\n","for adj in adj_Lists:\n","    # the documentation strictly says to convert the adjacency list to a contiguous list\n","    adj_Tensors.append(torch.as_tensor(adj.to_numpy(), dtype=torch.long).t().contiguous())   \n","\n","\n","#Iterate over each graph, make a Data object, then append to all our dataset\n","cntCorruptData=0\n","for i, adj in enumerate(adj_Tensors):\n","    d = Data(x=node_Features_List[i], edge_index=adj, y=torch.as_tensor([0]))  # <--- from the documentation.\n","    try:\n","        d.validate(raise_on_error=True)     # <--------- this line makes sure each input graph strictly follows the correct rules, to evade errors\n","    except:\n","        #print('DIMENSION ERROR')\n","        #print(f\"We have features for {len(node_Features_List[i])} Nodes \")\n","        #print(f\"But the adjacency list contains {max(set(np.array(adj[0,:])))} Unique Nodes\")\n","        cntCorruptData +=1\n","        continue\n","    allData.append(d)\n","\n","\n","print(f\"number of corrupted files due to missing node features for certain nodes = {cntCorruptData}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Train-Test-Split and data loader"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique classes {0, 1}\n"]}],"source":["\n","# Train Test Split\n","allData_train, allData_test = train_test_split(allData, test_size=0.3, shuffle=True)\n","allData_train, allData_val = train_test_split(allData_train, test_size=0.1, shuffle=True)\n","\n","\n","unique_classes = set()\n","\n","for d in allData_train:\n","    unique_classes.add(int(d.y.numpy()))\n","\n","print(f\"Unique classes {unique_classes}\")\n","\n","\n","#Our Data Loader\n","batch_size = 5\n","loader = DataLoader(allData_train, batch_size=batch_size, shuffle=True)\n","val_loader = DataLoader(allData_val, batch_size=batch_size, shuffle=True)\n","test_loader = DataLoader(allData_test, batch_size=batch_size, shuffle=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# GCN Class"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["GCN(\n","  (conv1): GCNConv(133, 128)\n","  (conv2): GCNConv(128, 128)\n","  (conv3): GCNConv(128, 128)\n","  (conv4): GCNConv(128, 128)\n","  (conv5): GCNConv(128, 128)\n","  (fc2): Linear(in_features=128, out_features=128, bias=True)\n","  (lin): Linear(in_features=128, out_features=2, bias=True)\n",")\n","Epoch: 001, Train Acc: 0.8391, Validation Acc: 0.9106\n","Epoch: 002, Train Acc: 0.8391, Validation Acc: 0.9106\n","Epoch: 003, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 004, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 005, Train Acc: 0.9060, Validation Acc: 0.9350\n","Epoch: 006, Train Acc: 0.8391, Validation Acc: 0.9106\n","Epoch: 007, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 008, Train Acc: 0.9340, Validation Acc: 0.9431\n","Epoch: 009, Train Acc: 0.9430, Validation Acc: 0.9593\n","Epoch: 010, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 011, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 012, Train Acc: 0.8436, Validation Acc: 0.9187\n","Epoch: 013, Train Acc: 0.8445, Validation Acc: 0.9187\n","Epoch: 014, Train Acc: 0.8445, Validation Acc: 0.9187\n","Epoch: 015, Train Acc: 0.8445, Validation Acc: 0.9187\n","Epoch: 016, Train Acc: 0.8445, Validation Acc: 0.9187\n","Epoch: 017, Train Acc: 0.9928, Validation Acc: 1.0000\n","Epoch: 018, Train Acc: 0.9837, Validation Acc: 0.9919\n","Epoch: 019, Train Acc: 0.9792, Validation Acc: 0.9919\n","Epoch: 020, Train Acc: 0.9928, Validation Acc: 1.0000\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","numClasses = len(list(unique_classes))\n","\n","model = GCN(hidden_channels=128, numClasses=2, numFeatures=133).to(device=device)\n","print(model)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-7)\n","#weights = torch.tensor(np.array([2.1, 1.1]), dtype=torch.float)\n","criterion = torch.nn.CrossEntropyLoss()\n","\n","for epoch in range(1, 21):\n","    train(model, loader, device, optimizer, criterion)\n","    train_acc = test(model, loader, device)\n","    val_acc = test(model, val_loader, device)\n","    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Validation Acc: {val_acc:.4f}')\n","\n","# Saving the model\n","with open('GCN_binary.pkl', 'wb') as f:\n","    pickle.dump(model.cpu(), f)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Testing"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00       460\n","           1       1.00      0.99      0.99        67\n","\n","    accuracy                           1.00       527\n","   macro avg       1.00      0.99      1.00       527\n","weighted avg       1.00      1.00      1.00       527\n","\n"]}],"source":["from sklearn.metrics import classification_report\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device=device)\n","\n","y_true = []\n","y_predicted = []\n","unique_classes = set()\n","for d in allData_test:\n","    y_true.append(d.y.cpu().numpy())\n","    \n","    pred,_ = inference(model, d.to(device=device), device=device)\n","    y_predicted.append(pred)\n","\n","\n","print(classification_report(y_true= y_true, y_pred= y_predicted))"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["number of corrupted files discarded = 0\n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (21694x33 and 133x128)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      4\u001b[0m modelPath \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mGCN_binary.pkl\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m      5\u001b[0m cve \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 10\u001b[0m classification, embDF \u001b[39m=\u001b[39m InferenceGCN (model\u001b[39m=\u001b[39;49mmodelPath, pathToUser_Edges\u001b[39m=\u001b[39;49mpathToUser_Edges, outputPath\u001b[39m=\u001b[39;49moutputPath, multipleFiles\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mfalse\u001b[39;49m\u001b[39m'\u001b[39;49m, cve\u001b[39m=\u001b[39;49mcve,npyPath\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, pathToUser_Nodes\u001b[39m=\u001b[39;49mpathtoUserNodes)\n\u001b[0;32m     13\u001b[0m \u001b[39mprint\u001b[39m(classification)\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\gcnHelpers.py:364\u001b[0m, in \u001b[0;36mInferenceGCN\u001b[1;34m(model, pathToUser_Edges, outputPath, multipleFiles, cve, npyPath, pathToUser_Nodes)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(model, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    362\u001b[0m     model \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(f)\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[1;32m--> 364\u001b[0m classification,Embedding \u001b[39m=\u001b[39m inference(model, inputPoint[\u001b[39m0\u001b[39;49m], device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    366\u001b[0m finalClassification \u001b[39m=\u001b[39m classification\n\u001b[0;32m    367\u001b[0m finalEmbedding \u001b[39m=\u001b[39m Embedding\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\gcnHelpers.py:321\u001b[0m, in \u001b[0;36minference\u001b[1;34m(model, data_point, device)\u001b[0m\n\u001b[0;32m    319\u001b[0m     edge_index \u001b[39m=\u001b[39m data_point\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    320\u001b[0m     batch \u001b[39m=\u001b[39m data_point\u001b[39m.\u001b[39mbatch\n\u001b[1;32m--> 321\u001b[0m     out, embedding \u001b[39m=\u001b[39m model(x, edge_index, batch)\n\u001b[0;32m    322\u001b[0m     pred \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m    323\u001b[0m \u001b[39mreturn\u001b[39;00m pred, embedding\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\gcnHelpers.py:150\u001b[0m, in \u001b[0;36mGCN.forward\u001b[1;34m(self, x, edge_index, batch)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, batch):\n\u001b[0;32m    148\u001b[0m     \u001b[39m# 1. Obtain node embeddings \u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index)\n\u001b[0;32m    151\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mrelu()\n\u001b[0;32m    152\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index)\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\torch_geometric\\nn\\conv\\gcn_conv.py:229\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[1;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    227\u001b[0m             edge_index \u001b[39m=\u001b[39m cache\n\u001b[1;32m--> 229\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin(x)\n\u001b[0;32m    231\u001b[0m \u001b[39m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001b[39;00m\n\u001b[0;32m    232\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate(edge_index, x\u001b[39m=\u001b[39mx, edge_weight\u001b[39m=\u001b[39medge_weight,\n\u001b[0;32m    233\u001b[0m                      size\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32md:\\ClassWork\\Guardista\\4-Features_Extraction\\venv\\lib\\site-packages\\torch_geometric\\nn\\dense\\linear.py:132\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        x (torch.Tensor): The input features.\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 132\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(x, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (21694x33 and 133x128)"]}],"source":["\n","pathtoUserNodes = r\"D:\\ClassWork\\Guardista\\BackEnd\\new_BE\\tmp\\tmp8\\output\\CFG_UserCode/nodes\".replace('\\\\', '/')\n","pathToUser_Edges = r\"D:\\ClassWork\\Guardista\\BackEnd\\new_BE\\tmp\\tmp8\\output\\CFG_UserCode/edges\".replace('\\\\', '/')\n","outputPath = r\"D:\\ClassWork\\Guardista\\BackEnd\\new_BE\\tmp\\tmp8\\output\\CFG_UserCode/nodes\".replace('\\\\', '/')\n","modelPath = 'GCN_binary.pkl'\n","cve = 'test'\n","\n","\n","\n","\n","classification, embDF = InferenceGCN (model=modelPath, pathToUser_Edges=pathToUser_Edges, outputPath=outputPath, multipleFiles='false', cve=cve,npyPath=None, pathToUser_Nodes=pathtoUserNodes)\n","\n","\n","print(classification)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
