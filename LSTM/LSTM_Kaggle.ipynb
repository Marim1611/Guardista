{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"TsbHUQteZK73"},"source":["# Vocab and Encoder\n","### Collecting our Vocabulary and training the encoder for the first time onlyyyy (it is done and you already have the encoder)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1) BB as a sequence (choose a cell from them)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-22T09:26:52.456596Z","iopub.status.busy":"2023-06-22T09:26:52.455971Z","iopub.status.idle":"2023-06-22T09:26:52.468133Z","shell.execute_reply":"2023-06-22T09:26:52.466155Z","shell.execute_reply.started":"2023-06-22T09:26:52.456565Z"},"id":"xHDjOzrlZK75","outputId":"253848a5-b234-447e-9bb4-e272e83aa376","trusted":true},"outputs":[{"data":{"text/plain":["'\\nimport LLNormalizer\\nimport os\\nimport json\\nfrom tqdm import tqdm\\nimport re\\nimport pickle\\nfrom sklearn.preprocessing import LabelEncoder\\nfrom sklearn.model_selection import train_test_split\\nimport numpy as np\\nimport multiprocessing\\nimport torch\\nimport torch.nn as nn\\nimport torch.optim as optim\\nfrom imblearn.over_sampling import SMOTE\\n\\n\\nvocabulary = set([\\'<pad>\\'])\\nmax_size = 0\\n\\n\\nX_train = []\\nY_train = []\\n\\nfor jsonfile in tqdm(os.listdir(\\'/kaggle/input/dataaa\\')):\\n    if(not jsonfile.endswith(\\'.json\\')):\\n      continue\\n    with open (\\'/kaggle/input/dataaa/\\'+jsonfile, \\'r\\') as f:\\n        jsondict = json.load(f)\\n    for k,v in jsondict.items():\\n        bbMerged = \\'\\n\\'.join(v[:-1])\\n        basicBlockContents = bbMerged\\n        basicBlockClass = re.findall(\\'\\\\d+\\', jsonfile)[0] #take the first digit in the filename as the CVE class\\n        tokenizedBB = LLNormalizer.Normalize_Tokenize_LLVM(basicBlockContents)\\n        max_size = max(max_size, len(tokenizedBB))\\n        vocabulary.update(tokenizedBB)\\n        X_train.append(tokenizedBB)\\n        Y_train.append(basicBlockClass)\\n\\nprint(f\"our Vocab Size = {len(vocabulary)}\")\\nprint(f\"Size of data = {len(X_train)}\")\\nprint(f\"max size is {max_size}\")\\n\\nencoder = LabelEncoder()\\nencoder.fit(list(vocabulary))\\nwith open(\\'encoder.pkl\\', \\'wb\\') as f:\\n    pickle.dump(encoder,f)\\n    \\n    \\n#Encoding each class\\nClassLabelEncoder = LabelEncoder().fit(Y_train)\\nY_train = ClassLabelEncoder.transform(Y_train)\\n\\n#Dont forget to use this encoder during inference\\nwith open(\\'ClassLabelEncoder.pkl\\', \\'wb\\') as f:\\n    pickle.dump(ClassLabelEncoder,f)\\n\\nnumClasses = len(set(Y_train))\\nprint(f\"We have {numClasses} Classes\")\\n'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# RUN this cell if you consider the BB as a sentence\n","\n","'''\n","import LLNormalizer\n","import os\n","import json\n","from tqdm import tqdm\n","import re\n","import pickle\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import multiprocessing\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from imblearn.over_sampling import SMOTE\n","\n","\n","vocabulary = set(['<pad>'])\n","max_size = 0\n","\n","\n","X_train = []\n","Y_train = []\n","\n","for jsonfile in tqdm(os.listdir('/kaggle/input/dataaa')):\n","    if(not jsonfile.endswith('.json')):\n","      continue\n","    with open ('/kaggle/input/dataaa/'+jsonfile, 'r') as f:\n","        jsondict = json.load(f)\n","    for k,v in jsondict.items():\n","        bbMerged = '\\n'.join(v[:-1])\n","        basicBlockContents = bbMerged\n","        basicBlockClass = re.findall('\\d+', jsonfile)[0] #take the first digit in the filename as the CVE class\n","        tokenizedBB = LLNormalizer.Normalize_Tokenize_LLVM(basicBlockContents)\n","        max_size = max(max_size, len(tokenizedBB))\n","        vocabulary.update(tokenizedBB)\n","        X_train.append(tokenizedBB)\n","        Y_train.append(basicBlockClass)\n","\n","print(f\"our Vocab Size = {len(vocabulary)}\")\n","print(f\"Size of data = {len(X_train)}\")\n","print(f\"max size is {max_size}\")\n","\n","encoder = LabelEncoder()\n","encoder.fit(list(vocabulary))\n","with open('encoder.pkl', 'wb') as f:\n","    pickle.dump(encoder,f)\n","    \n","    \n","#Encoding each class\n","ClassLabelEncoder = LabelEncoder().fit(Y_train)\n","Y_train = ClassLabelEncoder.transform(Y_train)\n","\n","#Dont forget to use this encoder during inference\n","with open('ClassLabelEncoder.pkl', 'wb') as f:\n","    pickle.dump(ClassLabelEncoder,f)\n","\n","numClasses = len(set(Y_train))\n","print(f\"We have {numClasses} Classes\")\n","'''\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 2) Function as a sentence"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T09:26:52.480507Z","iopub.status.busy":"2023-06-22T09:26:52.480046Z","iopub.status.idle":"2023-06-22T09:28:53.320888Z","shell.execute_reply":"2023-06-22T09:28:53.319900Z","shell.execute_reply.started":"2023-06-22T09:26:52.480465Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [01:54<00:00,  1.90s/it]"]},{"name":"stdout","output_type":"stream","text":["our Vocab Size = 4947\n","Size of data = 29444\n","max size is 53619\n","We have 2 Classes\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# RUN this cell if you consider the BB as a sentence\n","\n","import LLNormalizer\n","import os\n","import json\n","from tqdm import tqdm\n","import re\n","import pickle\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import numpy as np\n","import multiprocessing\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from imblearn.over_sampling import SMOTE\n","import CodePreprocessing\n","\n","\n","vocabulary = set(['<pad>'])\n","max_size = 0\n","\n","\n","X_train = []\n","Y_train = []\n","\n","\n","#I assume the dataset is inside a folder called llfiles\n","allLLfiles = os.listdir('llfiles')\n","for llfile in tqdm(allLLfiles):\n","    if(not llfile.endswith('.ll')):\n","        continue\n","   \n","    fn_head, fn_body = CodePreprocessing.functions_preprocessing('llfiles/'+llfile, '',writeJson=False)\n","    for i in range(len(fn_head)):\n","        classification = re.findall('\\{\\s+CVE(\\d+)', fn_head[i])\n","        if(not classification):\n","            classification = '0'\n","        tokenizedFN = LLNormalizer.Normalize_Tokenize_LLVM(fn_body[i])\n","        max_size = max(max_size, len(tokenizedFN))\n","        vocabulary.update(tokenizedFN)\n","        X_train.append(tokenizedFN)\n","        Y_train.append(classification[0])\n","\n","print(f\"our Vocab Size = {len(vocabulary)}\")\n","print(f\"Size of data = {len(X_train)}\")\n","print(f\"max size is {max_size}\")\n","\n","encoder = LabelEncoder()\n","encoder.fit(list(vocabulary))\n","with open('encoder.pkl', 'wb') as f:\n","    pickle.dump(encoder,f)\n","    \n","    \n","#Encoding each class\n","ClassLabelEncoder = LabelEncoder().fit(Y_train)\n","Y_train = ClassLabelEncoder.transform(Y_train)\n","\n","#Dont forget to use this encoder during inference\n","with open('ClassLabelEncoder.pkl', 'wb') as f:\n","    pickle.dump(ClassLabelEncoder,f)\n","\n","numClasses = len(set(Y_train))\n","print(f\"We have {numClasses} Classes\")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_mn5432mZK77"},"source":["# Data Loader\n","### We Load our dataset file by file from here, and each file is encoded before being stored in our X_train\n","### also we encode our classes from 190, 191 to 1 and 2 and so on\n","\n","### We have 2 loaders, one for BBs and the other for Functions\n","# Choose only 1 function from them to train"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T09:28:53.323864Z","iopub.status.busy":"2023-06-22T09:28:53.323165Z","iopub.status.idle":"2023-06-22T09:28:53.340548Z","shell.execute_reply":"2023-06-22T09:28:53.339497Z","shell.execute_reply.started":"2023-06-22T09:28:53.323828Z"},"trusted":true},"outputs":[],"source":["def BasicBlockDataLoader(folderPath, lastfileIndex, filesPerBatch=6):\n","    #Load the encoder for each token\n","    with open('encoder.pkl', 'rb') as f:\n","        encoder = pickle.load(f)\n","    padderLabel = encoder.transform(['<pad>'])[0]\n","    print(f\"padderLabel is {padderLabel}\")\n","\n","\n","    X_train = []\n","    Y_train = []\n","    cnt = lastfileIndex\n","    allJsons = os.listdir(folderPath)\n","    for jsonfile in tqdm(allJsons[lastfileIndex: lastfileIndex + filesPerBatch]):\n","        if(not jsonfile.endswith('.json') or (cnt-lastfileIndex > filesPerBatch)):\n","          continue\n","        cnt += 1\n","\n","        with open (folderPath+'/'+jsonfile, 'r', encoding='utf-8') as f:\n","            jsondict = json.load(f)\n","\n","        for k,v in jsondict.items():\n","            bbMerged = '\\n'.join(v[:-1])\n","            basicBlockContents = bbMerged\n","            basicBlockClass = re.findall('\\d+', jsonfile)[0] #take the first digit in the filename as the CVE class\n","            tokenizedBB = LLNormalizer.Normalize_Tokenize_LLVM(basicBlockContents)\n","            tokenizedBB = np.array(tokenizedBB)\n","            tokenizedBB = encoder.transform(tokenizedBB)\n","            X_train.append(tokenizedBB)\n","            Y_train.append(int(basicBlockClass))\n","    #Encoding each class\n","    #Load the Class encoder for each label\n","    with open('ClassLabelEncoder.pkl', 'rb') as f:\n","        ClassLabelEncoder = pickle.load(f)\n","    Y_train = ClassLabelEncoder.transform(Y_train)\n","    return X_train, Y_train, padderLabel\n","\n","\n","# same function but loads dataset function by function, not BB by BB\n","def FunctionDataLoader(folderPath, lastfileIndex, filesPerBatch=6):\n","    #Load the encoder for each token\n","    with open('encoder.pkl', 'rb') as f:\n","        encoder = pickle.load(f)\n","    padderLabel = encoder.transform(['<pad>'])[0]\n","   # print(f\"padderLabel is {padderLabel}\")\n","\n","    X_train = []\n","    Y_train = []\n","\n","    cnt = lastfileIndex\n","    allLLfiles = os.listdir(folderPath)\n","    for llfile in tqdm(allLLfiles):\n","        if(not llfile.endswith('.ll') or (cnt-lastfileIndex > filesPerBatch) ):\n","            continue\n","        cnt+=1\n","        fn_head, fn_body = CodePreprocessing.functions_preprocessing(folderPath+'/'+llfile, '',writeJson=False)\n","        for i in range(len(fn_head)):\n","            classification = re.findall('\\{\\s+CVE(\\d+)', fn_head[i])\n","            if(not classification):\n","                classification = '0'\n","            tokenizedFN = LLNormalizer.Normalize_Tokenize_LLVM(fn_body[i])\n","            \n","            tokenizedFN = np.array(tokenizedFN)\n","            tokenizedFN = encoder.transform(tokenizedFN)\n","            X_train.append(tokenizedFN)\n","            Y_train.append(classification[0])\n","\n","    \n","    #Encoding each class\n","    #Load the Class encoder for each label\n","    with open('ClassLabelEncoder.pkl', 'rb') as f:\n","        ClassLabelEncoder = pickle.load(f)\n","    Y_train = ClassLabelEncoder.transform(Y_train)\n","    return X_train, Y_train, padderLabel"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2023-06-22T09:28:53.342694Z","iopub.status.busy":"2023-06-22T09:28:53.341777Z","iopub.status.idle":"2023-06-22T09:28:53.356287Z","shell.execute_reply":"2023-06-22T09:28:53.355248Z","shell.execute_reply.started":"2023-06-22T09:28:53.342659Z"},"id":"bThrt_dHZK78","outputId":"d82f388e-42ed-4864-924b-e85b13779a19","trusted":true},"outputs":[],"source":["#print(X_train[0])\n","#print(Y_train[0:5])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Padding & SMOTE"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Padding and Preparing Tensors"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T09:28:53.359530Z","iopub.status.busy":"2023-06-22T09:28:53.359086Z","iopub.status.idle":"2023-06-22T09:28:53.369307Z","shell.execute_reply":"2023-06-22T09:28:53.368301Z","shell.execute_reply.started":"2023-06-22T09:28:53.359497Z"},"trusted":true},"outputs":[],"source":["def Pad_And_SMOTE(X_train, Y_train, max_size,padLabel, smoteflag):\n","    #Padding\n","    new_Train = np.zeros((len(X_train), max_size), dtype='int')\n","    for i,train in tqdm(enumerate(X_train)):\n","        new_Train[i] = np.pad(train, (0, max_size-len(train)), 'constant', constant_values=padLabel)\n","\n","\n","    #SMOTE\n","    if(not smoteflag):\n","        return new_Train, Y_train\n","    else:\n","        preserved_shape = new_Train.shape\n","        #print(f\"the original shape is {preserved_shape}\")\n","        # Reshape X_train to a 2D array of shape (n_samples, n_features)\n","        new_Train = np.reshape(new_Train, (new_Train.shape[0], -1))\n","\n","        # Reshape Y_train to a 1D array of shape (n_samples,)\n","        Y_train = np.reshape(Y_train, (Y_train.shape[0],))\n","\n","        # Apply SMOTE to oversample the minority class\n","        oversample = SMOTE()\n","        X_train_resampled, Y_train_resampled = oversample.fit_resample(new_Train, Y_train)\n","\n","        # Reshape X_train_resampled to a 3D tensor of shape (num_samples, sequence_length, input_size)\n","        num_samples = X_train_resampled.shape[0]\n","        sequence_length = max_size\n","        input_size = 1\n","        X_train_resampled = X_train_resampled.reshape(num_samples, sequence_length)\n","\n","        # Reshape Y_train_resampled back to a 1D tensor of shape (num_samples,)\n","        Y_train_resampled = np.reshape(Y_train_resampled, (Y_train_resampled.shape[0],))\n","        #print(X_train_resampled.shape)\n","        return X_train_resampled, Y_train_resampled"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Same Code but Optimized"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Loads the previous model (if present), continues training"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T09:28:53.371245Z","iopub.status.busy":"2023-06-22T09:28:53.370772Z","iopub.status.idle":"2023-06-22T09:28:53.461043Z","shell.execute_reply":"2023-06-22T09:28:53.459997Z","shell.execute_reply.started":"2023-06-22T09:28:53.371200Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","\n","class LSTMClassifier(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout, batch_first=True)\n","        self.fc = nn.Linear(hidden_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","        embedded = self.embedding(text)\n","        output, (hidden, cell) = self.lstm(embedded)\n","        hidden = self.dropout(hidden[-1])\n","        logits = self.fc(hidden)\n","        sentence_embedding = hidden.squeeze(0)  # Extract the final hidden state as the sentence embedding\n","        return logits, sentence_embedding\n","\n","\n","\n","def TrainModel(vocabulary, numClasses, new_X_tensor, Y_tensor, save_model_flag=True):\n","    \n","    # Define hyperparameters\n","    vocab_size = len(vocabulary)\n","    embedding_dim = 500\n","    hidden_dim = 280\n","    output_dim = numClasses # Number of classes\n","    num_layers = 3\n","    dropout = 0.5\n","    num_epochs = 10\n","    batch_size = 5\n","    learning_rate = 0.001\n","\n","    # Load and preprocess your data\n","    # ...\n","\n","    # Split your data into training, validation, and test sets\n","    xtrain, xval, ytrain, yval = train_test_split(new_X_tensor, Y_tensor, shuffle=True,test_size=0.3)\n","\n","    train_dataset = TensorDataset(torch.LongTensor(xtrain), torch.LongTensor(ytrain))\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","\n","    val_dataset = TensorDataset(torch.LongTensor(xval), torch.LongTensor(yval))\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","\n","    del(new_X_tensor)\n","    del(Y_tensor)\n","    del(xtrain)\n","    del(ytrain)\n","\n","    # Initialize your model and optimizer\n","    if(os.path.exists('LSTM.pkl')):\n","        with open('LSTM.pkl', 'rb') as f:\n","            model = pickle.load(f).to(device='cuda')\n","    else:\n","        model = LSTMClassifier(vocab_size, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device=\"cuda\")\n","    \n","    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # Define the learning rate scheduler\n","    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n","\n","    # for EARLY STOPPING\n","    endTraining=False\n","    # Train your model\n","    for epoch in range(num_epochs):\n","        model.train()\n","        train_loss = 0\n","        train_correct = 0\n","        train_total = 0\n","        for batch, (batch_text, batch_labels) in enumerate(train_loader):\n","            batch_text = batch_text.to(device=\"cuda\")\n","            batch_labels = batch_labels.to(device=\"cuda\")\n","\n","            optimizer.zero_grad()\n","            output, _ = model(batch_text)\n","            loss = criterion(output, batch_labels)\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to prevent explosion\n","            nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n","\n","            # Compute the number of correct predictions and the total number of samples processed so far\n","            _, predicted = torch.max(output, 1)\n","            train_total += batch_labels.size(0)\n","            train_correct += (predicted == batch_labels).sum().item()\n","\n","\n","            # Compute the accuracy for the batch\n","            batch_accuracy = 100 * train_correct / train_total\n","            # Print the accuracy for the batch\n","            print(f'Epoch {epoch+1}, Batch {batch+1}/{len(train_loader)}, Batch Accuracy: {batch_accuracy:.2f}%, Loss: {loss.item():.4f}')\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            #EARLY STOPPING\n","            #if(loss.item() < 0.4):\n","            #    endTraining=True\n","            #    break\n","        #if(endTraining):\n","        #    break\n","        \n","        \n","        # Compute the average training loss for the epoch\n","        train_loss /= len(train_loader)\n","\n","        # Evaluate your model on the validation set\n","        model.eval()\n","        val_loss = 0\n","        val_correct = 0\n","        val_total = 0\n","        with torch.no_grad():\n","            for batch_text, batch_labels in val_loader:\n","                batch_text = batch_text.to(device=\"cuda\")\n","                batch_labels = batch_labels.to(device=\"cuda\")\n","\n","                output, _ = model(batch_text)\n","                loss = criterion(output, batch_labels)\n","                val_loss += loss.item()\n","                _, predicted = torch.max(output.data, 1)\n","                val_total += batch_labels.size(0)\n","                val_correct += (predicted == batch_labels).sum().item()\n","\n","        # Compute the average validation loss and accuracy for the epoch\n","        val_loss /= len(val_loader)\n","        val_accuracy = 100 * val_correct / val_total\n","\n","        # Update the learning rate using the scheduler\n","        scheduler.step(val_loss)\n","\n","        # Print the training and validation losses and accuracy for the epoch\n","        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%')\n","\n","        \n","    #Save the model\n","    if(save_model_flag):\n","        with open('LSTM.pkl', 'wb') as f:\n","            pickle.dump(model,f)\n","    \n","    return model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Wrapping it up\n","### remember to choose the BB/function accordingly"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-06-22T09:28:53.462965Z","iopub.status.busy":"2023-06-22T09:28:53.462539Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 60/60 [00:46<00:00,  1.30it/s]\n","3885it [00:01, 3004.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1, Batch 1/544, Batch Accuracy: 0.00%, Loss: 0.7154\n","Epoch 1, Batch 2/544, Batch Accuracy: 50.00%, Loss: 0.5112\n","Epoch 1, Batch 3/544, Batch Accuracy: 66.67%, Loss: 0.2635\n","Epoch 1, Batch 4/544, Batch Accuracy: 75.00%, Loss: 0.0716\n","Epoch 1, Batch 5/544, Batch Accuracy: 80.00%, Loss: 0.0179\n","Epoch 1, Batch 6/544, Batch Accuracy: 83.33%, Loss: 0.0024\n","Epoch 1, Batch 7/544, Batch Accuracy: 85.71%, Loss: 0.0014\n","Epoch 1, Batch 8/544, Batch Accuracy: 87.50%, Loss: 0.0002\n","Epoch 1, Batch 9/544, Batch Accuracy: 88.89%, Loss: 0.0004\n","Epoch 1, Batch 10/544, Batch Accuracy: 90.00%, Loss: 0.0001\n","Epoch 1, Batch 11/544, Batch Accuracy: 90.91%, Loss: 0.0001\n","Epoch 1, Batch 12/544, Batch Accuracy: 91.67%, Loss: 0.0000\n","Epoch 1, Batch 13/544, Batch Accuracy: 92.31%, Loss: 0.0001\n","Epoch 1, Batch 14/544, Batch Accuracy: 92.86%, Loss: 0.0000\n","Epoch 1, Batch 15/544, Batch Accuracy: 93.33%, Loss: 0.0000\n","Epoch 1, Batch 16/544, Batch Accuracy: 93.75%, Loss: 0.0000\n"]}],"source":["fileBatchSize = 7\n","\n","for i in range (0,50,fileBatchSize):\n","    X_train, Y_train, padderLabel = FunctionDataLoader('llfiles', lastfileIndex=i, filesPerBatch= fileBatchSize)\n","    new_X, new_Y = Pad_And_SMOTE(X_train, Y_train, max_size, padderLabel, smoteflag=False)\n","    torch.cuda.empty_cache()\n","    new_X_tensor = torch.LongTensor(new_X)\n","    new_X_tensor.to(device = 'cuda')\n","    Y_tensor = torch.LongTensor(new_Y)\n","    Y_tensor.to(device='cuda')\n","\n","    model = TrainModel(vocabulary, numClasses, new_X_tensor, Y_tensor, save_model_flag=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Preparing for inference\n","### I ASSUMED THE INPUT FILE IS CALLED 'test.ll' you should change its name or directory"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["llfilePath = 'test.ll'\n","\n","\n","\n","# Load encoder\n","#Load the encoder for each token\n","with open('encoder.pkl', 'rb') as f:\n","    encoder = pickle.load(f)\n","padderLabel = encoder.transform(['<pad>'])[0]\n","print(f\"padderLabel is {padderLabel}\")\n","\n","\n","#Load model\n","with open('LSTM.pkl', 'rb') as f:\n","    model = pickle.load(f)\n","model.eval()\n","\n","classificationTest = dict()\n","Embedding = dict()\n","\n","# if input file is llvm file\n","\n","llTokenized_heads, llTokenized_bodies = CodePreprocessing.functions_preprocessing(llfilePath, '', writeJson=False)\n","for i in range(len(llTokenized_heads)):\n","\n","    # Pad to max size\n","    Test_arr = np.zeros(max_size, dtype='int')\n","    Test_arr[0: len(llTokenized_bodies[i])] = llTokenized_bodies[i]\n","    Test_arr[len(llTokenized_bodies[i]) : max_size] = padderLabel\n","\n","    torch.cuda.empty_cache()\n","    new_X_tensor = torch.LongTensor(Test_arr).to(device='cuda')\n","\n","    \n","    with torch.no_grad():\n","        model.eval()\n","        input_tensor = torch.LongTensor(new_X_tensor).unsqueeze(0)  # Add a batch dimension\n","        output, sentence_embedding = model(input_tensor)\n","        predicted_class = torch.argmax(output, dim=1).item()\n","\n","    print(f'Predicted class: {predicted_class}')\n","    print(f'Sentence embedding: {sentence_embedding}')\n","    sentence_embedding = sentence_embedding.detach().numpy()\n","    print(f'Sentence embedding: {sentence_embedding}')\n","\n","    classificationTest[llTokenized_heads[i]] = predicted_class\n","    Embedding[llTokenized_heads[i]] = sentence_embedding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-08fylOFZK7-","trusted":true},"outputs":[],"source":["'''\n","# Define hyperparameters and load your model\n","# ...\n","\n","# Preprocess your input sentence\n","# ...\n","\n","# Perform inference and extract the sentence embedding\n","with torch.no_grad():\n","    model.eval()\n","    input_tensor = torch.LongTensor(preprocessed_sentence).unsqueeze(0)  # Add a batch dimension\n","    output, sentence_embedding = model(input_tensor)\n","    predicted_class = torch.argmax(output, dim=1).item()\n","\n","print(f'Predicted class: {predicted_class}')\n","print(f'Sentence embedding: {sentence_embedding}')\n","sentence_embedding = sentence_embedding.detach().numpy()\n","print(f'Sentence embedding: {sentence_embedding}')\n","'''"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
